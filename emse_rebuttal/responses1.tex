\documentclass[8pt]{letter} % default is 10 pt
%\usepackage{newcent}   % uses new century schoolbook postscript font 
% the following commands control the margins:
\topmargin=-1in    % Make letterhead start about 1 inch from top of page
\textheight=8.8in  % text height can be bigger for a longer letter
\oddsidemargin=0pt % leftmargin is 1 inch
\textwidth=6.5in   % textwidth of 6.5in leaves 1 inch for right margin

\usepackage{booktabs}
\usepackage{letterbib}

\usepackage{natbib}
\usepackage{url}
\usepackage{xcolor}

%\usepackage[pdftex]{graphicx}
%\usepackage{textcomp}
%\usepackage{booktabs, tabularx}
%\usepackage{rotating}
%\usepackage[dvipsnames, gray]{xcolor}
%\usepackage[dvipsnames]{xcolor}
%\usepackage[round,sort]{natbib}
% \usepackage{multirow}


\begin{document}

\let\raggedleft\raggedright                % needed to get date flush left

\newcommand{\response}[1]{{\bf Response.} #1}

\newcounter{commentCounter}
\newcommand{\comment}[2]{
  \stepcounter{commentCounter}
  \vspace{2em}
  {\bf Comment R#1.\arabic{commentCounter}.} {\em #2}
  }
 
\begin{letter}{}

\address{
Muhammad Moiz Arif\\
EV 3.274, 1515 Saint-Catherine Street West\\
Department of Computer Science and Software Engineering, Concordia University\\ 
Montreal, QC, Canada H3G 2W1 \\
} 



\opening{Dear Editor and Reviewers:} 
 
\noindent Thank you for your insightful feedback and comments, both positive and constructive, and for allowing us the opportunity to improve our manuscript. We have taken each of your comments into consideration and made the appropriate changes and extensions to our manuscript. 

EDITOR comments:
	Overall the reviewers believe that the presented work is of great interest to the EMSE readers. All three reviewers are supportive of the publication of the work provided that the authors perform a major revision of the manuscript.	
	\\
	Reviewer 1 wants to see supporting evidence that VMs are widely used for performance testing. The reviewer points out there has been much work on performance testing -- the reviewer points out related work -- Nistor ICSE 13, Jin PLDI 12 that does not rely on VMs. Reviewer 3 tells you about the worrysome tendency that he sees in your paper to explain what it is that you are not doing, before explaining what you are doing. Please correct it in the revised version of the paper.
	\\
	The authors should clarify points in the paper. Reviewer 1 states that the the paper is not clear about how the three aspects of testing results can help to find performance problems. Also, the reviewer needs clarifications for the second analysis - the correlation between metrics - why is it useful? Reviewer 2 points out that your claim "To the best of our knowledge, the discrepancy between performance testing results in virtual and physical environments has never been studied.", is arbitrary and does not hold. Reviewer 3 also complains that the test environments are not quite clear and that the paper needs clear and precise research questions.
	\\
	Reviewer 3 Unclear research methodology, since it is not clear exactly what methodology is being followed. Moreover, the reviewer complains that the paper lacks actionable findings. The paper repeats in several places that there is performance of virtual and physical environments. That is interesting, but it is not actionable. The authors should work more on the paper to determine how their solution can contribute to the practice by offering actionable items based on your findings.
	\\\\
	We have now added motivation to our paper by giving examples and references of software like SugarCRM and BlackBerry's BES server. We have also added the related work on performance detection and bugs in our related work section. Furthermore, we have rephrased and have used a direct approach in our explanations.
	We have addressed that how our approach using three aspects of performance testing can help in identifying a performance discrepancy from a statistical point of view. Additionally, we have clarified an ambiguity present between our work and the related work. We now add that our work is first to examine the discrepancy between performance testing results in virtual and physical environment. A description of testing environments has now been added. We also have rephrased the implications and actionable findings of the paper. We not only emphasize that developers should not add overhead from virtual environments as is, but we also say that in order to normalize the performance metrics between the two environments prior approaches may not work. We propose an approach to minimize the deviance hence developers should leverage such an approach before processing performance testing results.
	
	
	
	
	
	

%We added a better motivation for our work. We also highlight how our work is different from the related literature provided as examples. We have provided how the three aspects of testing results impact the results from a practitioners point of view. Details for the testing environment especially the disk specs have been added.

Below, we include a description of the changes that we made to our manuscript with respect to each of the reviewer's comments. We denote the reviewers comments in italic typeface, and our responses follow below each reviewer comment.


%We thank the reviewers for their valuable feedback. This document includes a list of changes and rebuttal to the comments that were raised by the reviewers. Below, we include a description of the changes made to our manuscript with respect to each of the reviewers’ comments. We denote the reviewers comments in italic typeface. Our responses follow below each reviewer comment.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\vspace{1em}
\noindent {\Large {\bf Reviewer 1 Comment}}

\comment{1}{Is there any evidence supporting that VMs are widely used for performance testing? There has been much work on performance testing (e.g., Nistor ICSE 13, Jin PLDI 12) that does not rely on VMs. The paper should provide more details on the background and motivation for this study, such as why and in what circumstances VMs are used in performance testing. 
}


\response{Thank you for pointing this out. The motivation of this paper starts from our extensive collaboration with industrial practitioners, where we find that a large amounts of the performance testing are conducted in virtual environments. In order to confirm that our experiences with industry is not an exceptional case, we find online discussions and posts by developers and testers about performance testing in virtual environment \cite{performanceonvvirtual}\cite{stackoverflow}\cite{windowsserver}. In addition, we find that the developers of SugarCRM are documenting their experiences in conducting load testing (which has a big overlap with performance testing) on virtual environments \cite{sugarcrmexp}. \\
Based on these cases, we are confident that our experiences with our industrial practitioners who use virtual environment for performance testing is not an exceptional case. In fact, Reviewer 3 also support the fact that using virtual environments in performance testing is common. We add this in the introduction section.
}

%\response{Thank you for pointing this out. We found online discussions by developers and testers supporting our argument of testing across heterogeneous environments \cite{performanceonvvirtual}\cite{stackoverflow}\cite{windowsserver}. We also find a an experiment similar to our hypothesis where a web application(Sugar CRM) is tested for identifying performance issues between the physical and virtual environments\cite{sugarcrmexp}. There also exist VMware test labs to test an application and analyze performance metrics in a virtual environment \cite{vmware_test_lab}. In addition to that, we also highlight that Sugar CRM and Blackberry's BES server are offered with options of deployment on-premise or on cloud \cite{bbs} \cite{sugarcrm}. Furthermore, our experience with industrial partners speaks that virtual environments are used to test applications because of their flexibility. We have added a motivating example too to better motivate our paper.

%There are discussions online:
%http://stackoverflow.com/questions/8906954/can-you-use-a-virtual-machine-to-performance-test-an-application
%Even discussed here:

%https://social.technet.microsoft.com/Forums/windowsserver/en-US/06c0e09b-c5b4-4e2c-90e3-61b06483fe5b/performance-test-is-not-reliable-on-virtual-machine?forum=winserverhyperv

%This one is amazing:

%http://sqa.stackexchange.com/questions/7709/performance-testing-systems-on-virtual-machines-that-normally-run-on-physical-ma

%wOW man this is like a gift:

%http://www.webperformance.com/library/reports/Virtualization2/

%And this sugar crm supports both cloud and on-premise options. Plus BES.

%People even talk about building perf testing labs using vm
%http://searchvmware.techtarget.com/tip/Building-a-VMware-test-lab-How-to-obtain-and-interpret-performance-metrics

%Are these enough:)

%And it?s our expeirence with our industial partner over the years.

%You better add a subsection in your background to disccuss this.

%In order to better motivate our paper we added motivation example:

%A guy has a software system based on premise and cloud due to the constraints and flexib they test on vm correlation model building and finds that io is impacting their performance. Turns out vm io is being hampered. 

%Cite the io xen server. 

%If they knew there were discrepancy they wouldnt worry. Approaches to reduce noise.



\comment{1}{The study does not address the most important problem in performance testing, i.e., fault detection. Even if the discrepancy exist, thzere is no evidence showing that such discrepancy can affect testing effectiveness. The ultimate objective of performance testing is to find performance bugs. It would be more convincing if the authors can evaluate the discrepancy in finding bugs between VMs and physical environments. 
}


\response{We totally agree with this comment. The ultimate goal is to have investigate the impact on the detection of real world performance bugs. This paper is rather the first step to lay a ground to deeply understand such discrepancy.  Without such knowledge, directly evaluate on performance bugs would be rather treating the syndrome. With the knowledge of such discrepancy, we can better, in the future, understand the existence and magnitude of impact on detecting real world performance bugs. In addition, another contribution of our paper, besides the road towards real world performance bugs, is the effort of identifying approaches that may minimize the discrepancy. In particular, we find that the approach proposed by Nguyen et al.\cite{Nguyen:2012} for work loads in heterogeneous environments does not effectively reduce the discrepancy and we propose a new approach that is shown to be more effective. Our future work on evaluating the impact of real world performance bugs will based on the testing results that are processed with the reduced discrepancy.  
We add more discussion according to this comment in our conclusion.}

%Say this is the first step. First we need to know what is the descrepancy, how big and how to reduce.
%Then it makes sense to see the impact on fault and will be our future work. However, without understaing the nature of the descrpancy and directly see the impact on fault, you can?t really reason about results.



\comment{1}{ The paper is not clear about how the three aspects of testing results can help to find performance problems. Especially for the second analysis - the correlation between metrics - why is it useful? 
}

%To see the trends
%To the realtionships between metrics 
%To see the impact of metrics the all together. Many to many.

%\response{The investigation(s) in this work are based on the following 3 aspects:
%\begin{enumerate}
%	\item The first approach is used to identify the trends and distributions of performance metrics. As a result, we can look at the differences at a finer level between the two environments and not just by numbers only.
%	\item The second approach, was used to identify the change in the nature of relationship between performance metrics. We believe that a change in these relationships can effect the behavior of the subject systems in the two environments.
%	\item The third and final approach is used to see examine the impact of the metrics all together. This analysis also serves as the baseline for our future study i.e. fault detection.
%\end{enumerate}
%We have addressed and rephrased this in the journal.
%}


\response{The investigations in this work are based on the following 3 aspects:
	\begin{itemize}
	
	\item The first aspect to identify the trends and shape of the distributions of performance metrics. Due to the difference between testing environments, performance testing results are expected to be different in raw value. However, the shape of distribution and the trend should be similar. For example, when there is higher load, CPU increases. If in one environment, we observe the CPU has increasing trend while not seeing the same trend in another environment, we observe a discrepancy. Therefore, we use QQ plot and normalized KS tests to examine the differences in trends and shape of the distributions.
	
	\item The second aspect, is to identify the combination between two performance
	metrics. As claimed by  Cohen et al. \cite{cohen2004correlating}, combinations of performance metrics are significantly more predictive towards performance issues than individual metrics. We believe that a change in these combinations of relationships can reflect the discrepancy of performance in the two environments.
	
	\item The third aspect is used to see the combination of all performance metrics all together by constructing a statistical model. Similar approach has been adopted by Xiong et al. \cite{xiong2013vperfguard,cohen2004correlating}. This
	analysis also serves as the baseline for our future study i.e. fault detection.
	
\end{itemize}
	We have addressed and rephrased this in the new revision.
	
	In short, the three aspects are three ways that performance engineers may exploit to examine the performance testing results. We would like to set our study in such a context and see if the differences between virtual and physical environment would impact such analyses results, instead of simply checking the raw value of performance metrics, which is expected to always have differences.

	}



\comment{1}{In the related work section, instead of just describing the three types of analysis, the authors should relate the existing work to the proposed study. Does the discussed existing work rely on VMs? If it does, are there any problems caused by the discrepancy between VMs and physical environments?
}

%\textcolor{red}{Say explicitly where in the paper they have mentioned or cite papers where you can see this.}\\
%The analysis menioned is not domain specified so we are just testing in both the envion. And we find out that its dispcrepance.
%\response{We mention at the end of section 2.2:\textit{"Prior research focused on the overhead of virtual environments without considering the impact...and investigate whether such impact can be minimized in practice"}. The domain is not specified in most of the papers that we have mentioned as our related work. Hence, we tested in both the environments and concluded that the methodologies can not be applied as is.}

\response{Prior research does not explicitly mention the environment. Other prior research is performed on either virtual or physical environments only. For example, vPerfGuard \cite{xiong2013vperfguard} is only conducted in the virtual environment. However, none of them discuss the issue with being on both virtual and physical environments. This motivates our research. We added such discussion in our related work section to clarify.}.

\comment{5}{As virtualization becomes wildly adopted, many companies use virtual environment as their production environment to reduce operation costs. Does that invalidate the purpose of this study? What if an application is actually deployed in a virtual environment?
}



\response{Thanks for the comment. We agree that some of the applications may later deployed in a similar virtual environment. However, there still exist many scenarios that would value our study. As a first scenario, many systems have historical performance testing results that are based on the physical environments \cite{chenanalytics}. When comparing the new performance testing results and the old performance testing results, such discrepancy needs to be known. Second, software systems are often released both in virtual environment as cloud based services or on-premise. For example, SugarCRM \cite{sugarcrm} and BlackBerry Enterprise Service \cite{bbs} sell their systems both on-premise and on cloud based services. Ensuring the consistency between two solutions, or knowing the need (or to what extent) repeating performance tests in different environment is important to save resources. In addition, we also evaluate the discrepancy between performance tests both on virtual environment and with different virtual machines, which is also valuable performance testing and system deployment, both, are carried in virtual environments.
We clarify this and add more discussion about this in the introduction of our new revision.}
%\response{We agree however that would mean that we need to study the variance present in the virtual environment. We highlight a scenario where both the environments are used and not just only virtual. Like mentioned in response 1.1, some software have the option to run on premise. Particularly large software systems like CRM and BES. We clarify this in the new revision.}
%That sounds like another study about vm variance. But that?s not the focus of our paper. Some software needs to run on premise anyways. 

%We never talk about test and running in vm. We talk about a scenario where we use vm and physical. Particularly many large s/w systems offering vm and premise. Like CRM and BES. we clarify in the introduction.
%Detailed comments:

\comment{1}{Section 3.3: "the workload of the performance tests is varied periodically in order to avoid bias from a consistent workload" - how did it get varied.
}


%?Through the change in number of threads, threads represented users. As the users and number of requests sent/received are directly proportional in these software, eventually varying the workload.
%Although 7) the reviewer mentions that we have varied it according to the number of threads.

%Random workload, try to point it out. We clarify. 
\response{We varied it randomly by the number of threads and ensuring that the variation was identical between both the environments. We elaborate such detailed information in our new revision.}


\comment{1}{"The work-load variation was introduced by the number of threads." Why not consider other types of workloads, such as the amount of input data? Increasing the number of threads may also be used to speedup the performance. 
}

%What input data? If the reviewers mean the input data for the website It is an e-commerce website and the drivers for both systems come with predefined actioned. Log in, browse, buy, exit.


%This is a limitation we discuss in threads to validity. Only users=threads.
\response{Thanks for the comment. We agree that there exist many ways of varying workload. We opt to use the similar performance tests as prior studies that analyze performance testing results \cite{nguyen2012automated}, since we want to set our study in that context. Using other variation is planned in our future work. We discuss this limitation and the plan of future work in our new revision of the paper.}

\comment{1}{The quality of the performance tests could greatly influence the testing results. The paper should provide more details. Examples of performance tests can be useful. Also, how many performance tests are used in the study? Why does a test take so long (9 hours) to execute? Is running performance tests twice sufficient to reduce influence of randomness?
}

%Example of performance tests, I think I will put it under exploratory performance testing. Of course it is not done to see functional/non-functional anomalies. 
%The nature of the study itself requires 1 performance tests spread out over 9 hours. Concatenating multiple tests will only create noise in our data. Combination of workloads and usage scenario. We do not change performance tests, taken s/w as is. We do not want to impact any thing by making a special performacne test. 
%9 hours: I will site COR-PAUL who ran the test for 24 hours. The idea is to work on metrics under review statistically stable.  CITE: Jack.
%Its  about the sample size want to make sure enough data.
%We actually ran the test multiple times to reduce the randomness and to reach stable test output. But we took the best two runs and compared to them to evaluate the existence of randomness.
%Its a miscommunication. We actually ran at 3 times. And we do not guarantee that running it thrice will remove all the randomness. Add this to threads to validity. Limitation. 

\response{We apologize for the confusion. We answer the questions accordingly:
	\begin{enumerate}
		\item We use the same performance tests as used in the related studies \cite{jiang2009automated}\cite{malik2013automatic}\cite{nguyen2012automated}\cite{foo2010mining}. 
		\item Performance tests are typically run for a long period of time, similar to prior studies \cite{jiang2009automated}\cite{malik2013automatic}\cite{nguyen2012automated}\cite{foo2010mining}. The long length of performance tests reduces the risks that are introduced due to unstableness of the system, the missing coverage of certain load and the lack of statistically significant data for analysis. In future work, we can extend our study to run for even a longer period of time (e.g., 72 hours).
		\item Apologetically, this is a miss-reported number in the last revision. In fact, we ran each performance test three times in total. However, we do not guarantee that running it three time would completely eliminate the randomness. Our discussion on the variance between the same tests on the same environment is included in the discussion section.
		\\\\
		We have clarified all of the aforementioned points in the updated version of our manuscript. In addition, we discuss the quality of performance tests as a possible threats to validity of our findings.
		
	\end{enumerate}}




%\response{We answer the questions accordingly:
%	\begin{enumerate}
%		\item We use the same type of performance tests as used in the related studies \cite{jiang2009automated}\cite{malik2013automatic}\cite{nguyen2012automated}\cite{foo2010mining}. \textcolor{red}{Should I mention the exact type which was "exploratory performance testing(ref: Jack's TSE paper)"?}
%		\item In total, there are 3 performance tests used in this study, as mentioned in section 6.1: \textit{"In total, we had results from three performance tests"}
%		\item It was very necessary for this study that the systems are stable and the sample sizes are statistically significant. A longer run of the tests would ensure we have covered more data points, than the related studies \cite{jiang2009automated}\cite{malik2013automatic}\cite{nguyen2012automated}. In the future, we can extend our study to run for even a longer period of time.
%		\item We ran it 3 times in total. We do not guarantee that running it thrice would completely eliminate the randomness.
%		\\\\
%		We have clarify all of the aforementioned points in the updated version of our manuscript.
%	\end{enumerate}}


\comment{1}{Different performance tests may performance different functionalities (e.g., SQL query VS server restart). The functionalities should be evaluated separately. 
}

\response{We decided to test the subject system as a whole. Although testing a smaller unit of  functionalities may benefit in locating the performance issues. However the goal of the paper is to examining the discrepancy of the system as a whole, which is closer to the performance impact on real users. In our future work, we will separate different components of the system and conduct lower level, isolated performance tests, in order to investigate on impact on performance bugs.
We add discussion in the new revision of our paper.}



%I think this reviewer did not understand how our two software work and I can guess it by the questions. There is no need to evaluate different functionalities because atomically both the drivers are role-playing as a user. Log in - browse - buy - pay - logout. If the nature of all the transactions remains same based on the SQL query, I believe there is no need of evaluating it as different functionalities. 

%We use a combination of things. Whatever the test driver is based on. Test on single feature maybe system behaves in a different way but ours is based on compound exercising the system.


\comment{1}{Section 3.1: what is the size of each application?
}

\response{ The size of each application is added in the new revision.}


\comment{1}{On page 7, the design choice of combining metrics of two datasets is not justified. 
}

%To disregard the presence of multicollinearity or %counter-intuitive results such as simpsons paradox
%https://en.wikipedia.org/wiki/Simpson%27s_paradox 

%We considered it as one system. For a user its just a box. 
\response{Similar to comment \#9, we treat the system as a whole. The discussion of this choice is added into the new revision.}

\comment{1}{On page 7, realistically, interference on the real-world systems cannot be restricted like the one mentioned in the setup. The concern is that, by leaving out the system load, the statistical model might miss the opportunity to adjust to the real-world situation. Also, different assumptions about the system workload could affect the choice of the statistical model and thus perturb the prediction results.
}

\response{We agree that in real world, the systems may have different interference to impact their performance. However, in our experiments, we opt for a more controlled environment to better understand the differences without any interference, hence we can limit the chance that the discrepancy is from handling interference rather than the environments. Future work can be applied to investigate the performance impact from different environments by handling interference, with having the knowledge of environment discrepancy.
On the other hand, we agree that system load counters may illustrate valuable knowledge of the system. We include throughput metrics that are associated with system load. In addition, we will include more metrics in our future work.
We discuss the above points in our new manuscript threats section.}


\comment{1}{On page 15, what is the purpose of removing "metric that has a higher average correlation with all other metrics"?
}


%Explain.
%Dr. shang r function.

%We did it based on R. we removed the one which is more correlated to other metrics.
\response{We used such an approach to remove multicollinearity that is present between performance metrics. In fact, the approach is based on a popular statistical analysis \cite{cor_R}. In essence, whenever two metrics are found to be highly correlated, one of them needs to be removed. To determine which one to remove, the approach chooses the one that is more highly correlated with the rest of the metrics. We elaborate the description of our approach in the new revision of the paper.
	}

\comment{1}{On page 16, what regression model is used? On page 17, linear regression model is mentioned briefly. Why is a linear model chosen? Not until on page 19, the assumption of a linear relationship is mentioned. A brief writing of the design decision would be more appropriate. 
}
%Lrm. because prior work and easy to explain.

%Say you can use other models. You use linear model since it?s easier to explain, and also used in prior work, cite my paper icpe and the vperfguard

\response{We chose linear regression model as it is used in prior work \cite{xiong2013vperfguard}\cite{Shang:2015}. More importantly, the linear model is more straightforward to explain compared to that other model. Hence, it is easier to interpret the discrepancy that are illustrated by the model. We discuss the decision in our revised manuscript.}

\comment{1}{R-squared is used without explanation. If 10-fold cross validation has an explanation, R2 may deserve one too.
}

\response{We provide this explanation in the revised manuscript.}

\comment{1}{On page 18, "good model fit (66.9\% to 94.6\%)", is that the absolute percentage error?
}

\response{Sorry for the confusion, the values are the R\textsuperscript{2} of our models. We elaborate our text to avoid such confusion in the new revision.}

\comment{1}{There is much work on performance testing and bug detection, which should be discussed in related work. 
}

%Yes
%Look for performance bug detection. 4 or 5 papers. Look at the reviewers example.

\response{We include the new discussion in more related work about performance testing and bug detection. The list of newly added related work include:
	\begin{itemize}
		\item Nistor et al.: Toddler: Detecting Performance Problems via Similar Memory-access Patterns  in ICSE '13 \cite{Nistor}.
		\item Jin et al.: Understanding and Detecting Real-world Performance Bugs in PLDI '12 \cite{Jin}.
		\item Nistor et al.: Discovering, reporting, and fixing performance bugs in MSR '13 \cite{nistor_2}.
		\item Tsakiltsidis et al.: On Automatic Detection of Performance Bugs in ISSREW '16 \cite{Tsakiltsidis}.
		\item Malik et al.: Automatic Comparison of Load Tests to Support the Performance Analysis of Large Enterprise Systems in CSMR '10 \cite{h_malik_p_bugs}.
		\item Zaman et al.: A qualitative study on performance bug in MSR '12 \cite{zaman_p_bugs}.
		
	\end{itemize}
	
	
	}
 

\comment{1}{If the trace data can be made public, others may use it to replicate the experiments.
}

\response{Thanks for the suggestion. We have now made the data public.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{2em}

\noindent {\Large {\bf Reviewer 2 Comments}}

\comment{2}{This paper is championing the fact that there is a lack of compatibility or similarity between the performance metrics obtained from virtualized and physical environment. The purpose is secondary; whether the performance metrics collected are the solely the result of performance testing (load, stress, smoke, tortures or capacity testing) aka active testing OR collected during passive testing, i.e., field monitoring/testing for various purposes, i.e., anomaly detection, continuous validation of workloads, for future long and short term forecasts.
	
	
Therefore, the claim "To the best of our knowledge, the discrepancy between performance testing results in virtual and physical environments has never been studied.", is arbitrary and does not hold.
	
Some work exists in that have implicitly compared the difference between the metrics harvested from the VMs and Physical machines, however, performance testing was not their main focus. A few explicit research efforts exist to gauge the difference between metrics collected from physical and virtual environment. I am pointing to one such work conducted by Netto from PUCRS and Sadd from Dell "Evaluating load generation in virtualized environments for software performance testing". They conducted several load tests on both virtual machines and physical machines.}

%\underline{Context}: Prior studies show that virtual environments are widely exploited in practice... "To the best of our knowledge, the discrepancy between performance testing results in virtual and physical environments has never been studied" when compared to related literature.

%say you compare it using typical anlaysis of perf tests. and cite it.\\\\
%\response{After reading the work mentioned we identified the authors approach is based on comparing the raw values collected via load testing. However, we analyse the results from a software performance test?s perspective \cite{jack_tse}\cite{nguyen2012automated}. We have added this work in our related work.}

\response{Thank you for your feedback. 
	
	We agree that there exist prior studies such as the one mentioned above that examine performance counters from both environments. However, the comparison is rather simple. For example, in the above mentioned study, the authors directly compare the metric values. In fact, difference exists between any two tests, even from the same environments, yet statistically, they may not. Therefore, we examine the performance testing results in the context that when performance engineers use different statistical analyses on these results and see whether the difference between vm and physical would impact the statistical analyses results. 
	
	We discuss the above difference between our work and prior studies in related work section. In addition, we have revised our manuscript to say our work is one of the first works that examine the discrepancy between performance testing results in virtual and physical environments. 
	}


\comment{2}{However, the use of VMs may introduce extra overhead (e.g., a higher than expected memory utilization) to the testing environment and lead to unrealistic performance testing results...Why testing over virtualized lead to unrealistic PT results?
	}
\\\\
%The overhead is very complex not just see the overhead and evaluate the dicrepancy. b/c the overhead is not straight forward. You better read the vm overhead paper and cite them and say the overhead is rather complex. So you can;t just add up the overhead direct on top of the results.
\\\\
\response{Prior studies on systems and hardwares have investigated the overhead of virtual machines. For example, Huber et al. try to build a performance model to predict performance of applications that are migrated from a native system to a virtual environment or from a virtual environment to a new one \cite{huber2011}. There are similar studies which indicate that the overhead in virtual environment may hamper the results of performance tests \cite{brosig2013}\cite{menon2005}. However, it is found that such overhead cannot be simply added up to the existing system load, while it instead brings confounding effect on the system performance overall. Having known the existence of such overhead, interpreting performance testing results is not straightforward.}
%\response{The overhead in a virtual environment can not be simply added on with the results and analyzed. It is rather complex to calculate and evaluate the discrepancy. That is why \textit{Huber et al.} try to build a performance model to predict performance of applications that are migrated from a native system to a virtual environment or from a virtual environment to a new one \cite{huber2011}. There are similar studies which indicate that the overhead in virtual environment may hamper the results of performance tests \cite{brosig2013}\cite{menon2005}.  }

\comment{2}{ Our findings show that practitioners cannot assume that their performance tests that are observed on one environment will necessarily apply to another environment... Is there any evidence to back up the current practice(s) in which practitioner generalize the result of one environment to another (phy to Vir), especially of the large scale software, you mentioned in the paper?
	}
%	Vm and on premise. 
	
%	In the light of this:
%	http://www.bluelinkerp.com/hosted-onpremises-solutions/
%	You can also mention about BES, which is also both on cloud and on premise.
%	if you can find some thing to say, some apps can be hold both on cloud and on premise, then they may not have the effor to testing for each environment
\response{
	Thanks for the question. Such a question is also raised by reviewer 1. Please refer to our responses to comment 1.1 and 1.5.
	In short, we find that the use of virtual environment in performance testing is not an exceptional cases but rather common in practice. In addition, there exist scenarios that both physical and virtual environments are used for systems (e.g., services provided both on cloud and on-premise).}

\comment{2}{Exploring, identifying and minimizing such discrepancy will help practitioners and researchers understand and leverage performance testing results from virtual and physical environments...Why would they *LEVERAGE* performance testing results from virtual and physical and to achieve *WHAT* purpose. Bench-marking folks won't like this idea
\\\\
Counter narrative: Many companies also try to virtualize their load generation infrastructure which seems like a good idea for maintenance and elasticity reasons. However, they defiantly do tests on physical environment for the show-off purpose of their performance. This is a fact that you can squeeze the most performance out of physical environment. Nevertheless, especially for SaaS infrastructure, or to cater the need of stakeholders and large clients, virtualized environment are used and performance results obtained are attributed to specific virtual environment. That's why the benchmark teams exists in large enterprise that catalog the performance under different virtualized environment. Application of findings from one environment to another is not a usual practice}.

\response{Thanks for the question. Such a question is also raised by reviewer 1. Please refer to our responses to comment 1.5. In short, the practitioners may need to compare to historical performance testing results or hosting the system both as cloud service or on-premise.}


%\response{\textcolor{blue}{better look for some examples online, like web posts.}}


\comment{2}{Since the authors have framed the paper as an empirical study, it is very important to provide the necessary information in the paper for replication purpose. Replication, allows other researcher to validate authors claim(s) and in cases, compare it with their own techniques/cases and findings. What are workload parameters are used in this paper for DS2 and for Cloud Store using JMeter? }


\response{We will share our data and our load driver configuration for replication purposes.}

\comment{2}{Did you used the same hardware for setting up the virtual environment?}
	
\response{Yes, we have used the same hardware for setting up the virtual environment. We clarify this in the new revision.}

\comment{2}{Among the three machines, on which machine you were running perfmon agent (remote collection)? OR you were running it on all three machines?
	
	}
\response{
	We use perfmon to monitor on the two machines that run web(app) server and the database server. We do not monitor performance of the machine but instead the performance of the web application process and the database process directly, in order to minimize the influence of the perfmon. We clarify this in the new revision. }

\comment{2}{How did you ensured that environment remained constant for each performance test? For example after few tests, the disk may get full hence IOPS can get impacted. 
	}
	
\response{We restore the environments and restart the systems before conducting every test. We add such clarification in the new revision.}

\comment{2}{ For both the Q-Q plots for D2 and Cloud store, the metrics do not have the same trend. Did you used the samples of the performance tests results when the test was in equilibrium? If yes how did you ensured that?
	}
	
\response{Yes, we waited till the point of stability before processing. We achieve this by not including data from the beginning and ending of a test (10 minutes each end). This was a consistent practices across all our tests.}


\comment{2}{Also, before calculating the correlation among the performance metrics from virtual and physical environments, did you removed ramp-up and ramp-down observations of the performance metrics for a test? The system is usually not stable during warm-up and cool-down period. Ramp-up and down periods, for a test repeated multiple time under a given workload, and constant environment, many not necessary be correlated to each other.
	}
	
\response{We completely agree with the comment and the sample was taken after the warm up, the ramp-up and before the cool-down period.
	We ran the warm up period for 2 minutes. We remove the recorded performance metrics during this time frame. As explained in comment 2.27, we removed the recorded data during our ramp-up and cool down period in order to ensure equilibrium.}

\comment{2}{What is the implication of this study?
	}
	
%Future resarch should investigate how to reduce the discrepancy and to be careful when working in two environments. 

\response{There exist three actionable implication from this study:
	\begin{itemize}
	\item Developers cannot assume an straightforward overhead from the virtual environment, (such as a simple increasement of CPU).
	\item Prior approach that are proposed to normalize performance testing results with different loads may not work between physical and virtual environments.
	\item We propose an approach that may minimize the discrepancy between performance testing results from virtual and physical environments. Developers should leverage such an approach before processing performance testing results from virtual and physical environments.
\end{itemize}
	All above discussion is included now in our new revision.}



\comment{2}{In order to assist the practitioners leverage performance testing results in both environments, we also investigate ways to transform results from virtual and physical environments and performance metrics based on deviance may reduced [REDUCE] the discrepancy between performance metrics... Weird sentence, unless am reading between lines.}

\comment{2}{...such challenges, virtual environments (i.e., VMs) are often leveraged for performance testing [8,8,47]... why "8" is repeated in the ref?}

\comment{2}{...ents, such overhead would not significantly impact on the practitioners who examine the performance testing results...significantly impact WHAT on the practitioners?}

\comment{2}{...paper, we perform a study on two open-source systems, DS2 [13] and CloudStore [10], where performance tests are conducted on[USING] virtual and physical environments}


\comment{2}{we study whether the performance metric follow[S] the same shape of [THE] distribution and the same trend in virtual and physical environments.}


\comment{2}{...which can lead to [A] different set of conclusions.}

\comment{2}{For example, [THE] virtual environment has a CPU's utilization spike at a certain time [,] but the spike is absent in the physical}
	

\comment{2}{...there exist[S] a plethora of VM software}
	

\comment{2}{...high when we normalize by [THE] load as per Equation}


\comment{2}{In the reference section, there is an extra '24' hanging at the end of the page}

\vspace{2em}


\response{All above typos are fixed, except for comment 2.39, which is not a typo but the page number. We also proofread it again to eliminate language issues.}

\comment{2}{Overall, interesting research and paper was a joyful read.}

\response{Thank you!}
	
	

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{2em}

\noindent {\Large {\bf Reviewer 3 Comments}}

\comment{3}{The test environments are not quite clear.  This concern is minor because it could be fixed with complete descriptions of the hardware and virtual environment configuration.  For example, is the physical server disk setup a RAID?  SSD?  SATA/NVMe?  Is the network traffic generated locally or is it simulated from another machine (which would be influenced by the network hardware)?
	Perhaps more importantly, what is the disk setup on the virtual machine?  One of the findings in the paper is that the I/O metrics differ more than the CPU/memory metrics when compared to the physical machine.  That is definitely not surprising if the VM is configured to use VDI or another virtual disk, given the overhead in mimicking a drive within an existing filesystem.  It is possible to use disk passthrough in Virtual Box and other VM software.  In that case, a physical disk is passed by the host OS directly to the guest OS.  That setup tends to be much much closer to native speed.
	}
	
%\response{Thank you for your feedback in helping us making it a stronger manuscript. We have now provided a detailed description of environment configuration. \textcolor{red}{about the network traffic, it was simulated what can I say about this by not being hampered by the concordia network?}. One of our focus when carrying out the experiments was to keep the environment setup as close to real world as possible. There are online posts as why disk pass through is not used any more in the real world environment \cite{diskpassthrough}. Due to lack of flexibility and lack of support we decided to use a virtual disk.}

\response{Thank you for your feedback in helping us making it a stronger manuscript. We have now
	provided a detailed description of environment configuration in the new draft.
	The network traffic was generated on another machine. As we were well below the capacity the network overhead did not make a difference. In addition, as the network setup was same for both of our environments the effect would cancel out during the analysis.
	One of the concern was about using disk passthrough. We opted to not use disk passthrough mainly due to the fact that disk passthrough was not enabled in our experiences of collaborating with our industrial partners, which primarily motivates this work. The main reason is the portability issues, since practitioners want to quickly deploy an existing vm image that's designed for performance testing and start performance tests right away, there are other discussions online about why disk passthrough may not be a good idea\cite{diskpassthrough}. Based on such information and mainly practices in our industrial collaboration, we opted not to enable it.
	We elaborate our choice in the new revision.}



\comment{3}{Unclear research methodology.  At several points, the paper is not clear on exactly what methodology is being followed.  One potential deal breaker for this paper is that the test scenarios for the virtual and physical environments are "similar" so the distributions should have the same shape (first sentence on page 10).  The test scenarios should be identical, not just similar.  Similar implies that the authors created a test scenario for each environment using the same tools.  They should instead have created just one test scenario.  If the test scenarios are not the same, then the discrepancy could obviously be due to a different scenario.  One way to ensure that the tests are the same is to record the network traffic of one run and use the recorded traffic as input for the next run.  In a virtual environment, one can even use the same time and date settings.
	\\\\
	The paper needs clear and precise research questions.  The paper jumps into methodology and analysis without clearly explaining what is to be analyzed.  For example consider section 4.1, "Examining individual performance metrics."  It is not explained exactly what an "individual" metric is.  I had to try to understand it from reading the Approach subsection in 4.1.  The concept is not that difficult; the section is just comparing, e.g., a CPU metric in one environment with the same metric in the other environment.  It just needs to be explained more clearly up front.  Some of the language is confusing, such as "intuitively the scales of performance metrics are not the same."  I am not sure if the sentence refers to scales not being the same across metrics or across environments.
	\\\\The authors have tendency to explain what they are not doing, before explaining what they are doing.  That is confusing because it makes it difficult to understand the methodology.  For example in Section 4, "We do not predict... Instead, our experiments are set in..."
	\\\\
	All of these issues with unclear research methodology could be fixed by adding clear research questions and a methodology for answering each question.  The bigger problem here is that the methodology is unclear enough that it might be obscuring more important problems under the surface.
	
	
	}

\response{Thanks for your suggestion, we improve our explanation of the empirical study in our new draft to avoid confusions by having 3 research questions. We also rephrase the texts to improve the ease of understanding our approaches.
	
	In particular, we did not adopt an approach that monitor the network traffic to control load. Our experiments are trying to replicate the practice of our industrial practitioners, where VMs hosting performance tests are pre setup and the load drivers, with the same configurations, are setup in the VM. Practitioners will start the performance tests by initiating the load driver without really controlling the network traffic. We agree that leveraging a more controlled experiment is beneficial to further understand the discrepancy.
	
	We clarify our experiments in the new draft and discuss the limitation of our approach. We also discuss the practice of conducting performance testing using virtual machines in order to better motivate the study and motivate our experimental choices. Please refer to comment 1.1 and 1.3. We put conducting a controlled experiment in our future plan as part of the conclusion of the paper.}

\comment{3}{Lack of actionable findings.  The paper repeats in several places that there is a discrepancy between the performance of virtual and physical environments.  That is interesting, but it is not actionable.  As a programmer or tester, I am not sure how to use this information.  The paper does not give an explanation of how large the difference is, except by presenting the charts and figures.  For example, in the last paragraph on page 9, "By looking closely at such metrics, we find..."  Earlier it states that the "lines on the Q-Q plot are not close."  I can see what the authors mean by looking at the figures, but is that a big difference?  As a tester, I am already aware that the virtual environment is not a perfect reflection of a physical one.  What I am wondering is, is the difference big enough that I need to worry about it?  The paper does not quite answer that question.  Perhaps the paper could go into an example to show how the results might affect decision-making, to give some context for understanding the results.}

\response{Thanks for pointing this out. This comment was also raised by reviewer 2. Please refer to our response to the comment R2.29. We add the discussion of such actionable implications in the new draft of this paper. We have also added a motivating example.}
	
%\vspace{1em}
%\noindent {\Large {\bf Reviewer 1 Comment}}

%\comment{1}{This article studies the user reviews for top mobile Apps in the Google Play Store and compares its findings with those on IOS apps.  The study is quite impressive in the scale as they collected user reviews of 10,000+ top apps on a daily basis over a period of two months. Their findings are quite interesting to learn, which have some commonalities as well as differences from previous results.  It would be even more interesting if the analysis can have something on the topics and content summaries of the user reviews, e.g., ratings, popular words in the reviews, and so on.  Currently it is mostly on the statistics, e.g., number of reviews.
%}

%\response{Thanks for the suggestion! We studied the topics in the top 100 most reviewed apps and investigated the global and local topics in app reviews. The study results are briefly discussed  in the paper as an infobox due to the imposed word count limits on the actual article. We hope this infobox satisfies your request while still ensuring that we do not exceed the imposed word count limits.
%}


%\noindent {\Large {\bf Reviewer 2 Comment}}

%\comment{2}{
%In this paper, the author conducted data analysis over the reviews on Mobile Apps in Google playstore. They found some interesting facts, such as a very small number of Apps received much more reviews than the others. Though the authors argued strongly that their findings are different from Pagano and Maalej's and Hoon et al.'s observations, in fact, there is no New finding in this study. The real difference is that the previous study was conducted over Appstore and this one was conducted on Google Playstore. Thus, I do not think this paper should be published without a new finding.
%}

%\response{App stores (and in particular user and developer interactions through app reviews) are a whole new domain of study in computing today. Our study is important in that it raises a warning flag about the importance of looking across such stores and that the computing community cannot just assume that the observed patterns are universal. Instead according to our study, there are differences between stores. For example, while prior studies highlight that many iOS developers are overwhelmed with reviews, we note that this is not the case for most Android apps. Furthermore, we are the first to dig deeper into the relation between app reviews and other meta data (as pointed by reviewer 3). Table 2 details the key differences between our study and prior studies
%}


%\noindent {\Large {\bf Reviewer 3 Comments}}

%\comment{3}{
%On the other hand, the study remains mostly at the descriptive level, leaving out implications, interpretation, and discussions. I would really appreciate more context in a discussion section on why the study is important and which implications it has, e.g. for automated extraction of topics, triaging etc. as the beginning of the manuscript might ``promise''.
%}

%\response{The greatest implication and take-home message of this paper is that practitioners and researchers on mobile app should be mindful of the differences among different app stores. Assumptions of having the same observations/patterns across different apps stores may not hold. With the awareness of the differences, techniques that are designed to assist mobile app developers should be optimized for different app stores. Similarly, researchers should examine whether other empirical findings do hold across app stores.
%We added the discussion of the implication of our paper to the %new revision of the paper.
%}


%\comment{3}{
%Related work is complete, but could be set more into the %context of this manuscript.
%}

%\response{The related work is set into the context of this %paper in the follow 3 aspects
%\begin{enumerate}
%	\item The importance of mobile app user reviews motivates our study.
%	\item The findings of this paper ascertains the context of prior techniques that analyze mobile app user reviews.
%	\item Prior empirical studies on mobile app user reviews focus on iOS apps, while our findings on Google Play Store are different.
%\end{enumerate}
%These points are highlighted in bold in Section ``Mobile App Analytics'' in the new draft.
%}

%\comment{3}{
%Due to the app selection and sampling process, the presented results are only valid for free, established, popular apps - a fact that should be carefully noted throughout the manuscript.
%}

%\response{We noted such limitation in the new revision of the paper.
%}


%\comment{3}{
%Language and typos should be checked.
%}

%\response{Fixed.
%}



%\comment{3}{
%Introduction
%- Yet little is known about the reviewing dynamics of the %Google Play Store =\textgreater This seems a bit exaggerated.
%}

%\response{The sentence is rephrased in the new draft. 
%}

%\comment{3}{
%The number of downloads and releases impact the number of received reviews =\textgreater Should be "correlates with" instead of "impacts". In the case of downloads and releases - other than categories - the authors do not know which influences which, so a causal relation is not given per se.
%}

%\response{The sentence is rephrased as suggested.
%}

%\comment{3}{
%Mobile App Analytics
%- ll. 47ff: The related work (apart from the two studies that are also shown in the table) could be presented in more detail. What were concrete outcomes? How do they concretely relate to the authors' work?}

%\response{Please refer to our response to comment 3.2.

%The related work are set to present three aspects as our response to comment 3.2. We added more information about the outcomes of the related work. Related work does not consider that the assumptions of having the same observations/patterns across different apps stores may not hold. Our work makes the implication and take-home message that practitioners and researchers on mobile app should be mindful of the differences among different app stores. With the awareness of the differences, techniques that are designed to assist mobile app developers should be optimized for different app stores. Similarly, researchers should examine whether other empirical findings do hold across app stores.

%}

%\comment{3}{
%Studied Apps
%- The fact that only 500 reviews per day were sampled at max is a pity. This does not really allow to describe the full spectrum of how reviews distribute. However, the median \_should\_ still be stable, given that the authors only found few apps with more than 500 reviews.
%}

%\response{Thanks for the suggestion. We find the median number of reviews per app per day to be 0. We elaborate such finding in table 2 in the new revision of the paper.
%}

%\comment{3}{
%- Only top apps are studied =\textgreater this reduces the subjects and generalizability to popular apps
%- Only ``stable'' (old) apps are studied =\textgreater this reduces the subjects and generalizability to popular, established apps
%- Only free apps are studied =\textgreater this reduces the subjects and generalizability to popular, established, free apps.
%}

%\response{We agree that our findings are limited to top, stable and free apps in Google Play Store. We mention such limitations in the new revision of the paper.
%}

%\comment{3}{
%- Why would spam reviews be a problem? I don't see the argument that there are usually few reviews as an argument for not having spam. Spammers might choose relevant apps. Also I don't think it is relevant that you must login before reviewing, as this is no limitation for spammers. And: In the end there is spam, according to [3] (the study is for iOS, but there you have to login as well).
%}

%\response{The paragraph is removed from the new revision.
%}

%\comment{3}{
%Our Findings
%- I don't really see much on the beanplots. Maybe a traditional boxplot with whiskers and outliers would give a better impression?
%}

%\response{We tried to use boxplots as well. For figure 1(a) and figure 1(b): All other apps, box plots and bean plot would show similar information since the distribution is highly skewed (more than half of the data is 0). On the other hand, the figure 1 (b): 100 most reviewed apps, the bean plot can show the density of apps in different amount of reviews. For example, we can observe that most of the top 100 reviewed apps have around 10,000 reviews during the studied period. Such information can't be shown by boxplots. However, if the reviewer feels that box plots are more appropriate, we can replace them in the next revision.

%}

%\comment{3}{
%- ... ``4,275 reviews in a single day such large numbers would lead to increasing the overall reported average of the received reviews on a daily basis.'' =\textgreater Yes, but not necessarily the median!
%}

%\response{We do not mean that the the findings from prior study on Facebook represent the normal situation (like median) of other apps in iOS store. We try to explain that some apps, like Facebook, may have way over 500 reviews per day. Therefore, our limitation of seeing only 500 reviews may be the reason that we observe lower amount of reviews compared to prior studied. However, only 20 apps have more than 500 reviews per day. Therefore, we still calculate the average number of reviews in a single day in order to compare to prior study results.

%To further address such limitation, we check the median number of reviews in a single day for each app. We find that the median number is 0. Although Hoon et al. in prior study reports that the median number of reviews per year is 50 and 30 for free and paid apps, respectively, we cannot directly compare such results (median 50 or 30 reviews per year) with our result (median 0 review per day). We report the median number of reviews for each app per day in table 2.
%}

%\comment{3}{
%- Figure 1b should read ``All other apps''. It is wrong as it is now.
%}

%\response{Fixed.
%}

%\comment{3}{
%- Most apps receive few feedback and thus would not benefit from automated analyses. =\textgreater 20 reviews per day is already an amount where you would need a person for reading, triaging, detecting duplicates etc. So I would not subscribe this actually.
%}

%\response{We calculate the number of tokens from all the reviews of everyday for every app. The median number of words for app to receive for one day is 46. Such results confirm that most apps do not receive large amounts of feedback. We include such results in the new revision of the paper.
%}




\vspace{5mm}

\noindent Again, we thank all of you for your valuable feedback, which has made
this a stronger manuscript. We look forward to hearing your feedback on the
updated manuscript.

 \vspace{2mm}
 
\noindent{Sincerely,}\\
\noindent{Muhammad Moiz Arif, Weiyi Shang, \& Emad Shihab}\\
%\noindent{\tt{\{tsehsun, sthomas, hemmati, mei, ahmed\}@cs.queensu.ca}}
\end{letter}
 
%\bibliographystyle{natbib}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
\bibliographystyle{plain}
%bibliographystyle{spphys}       % APS-like style for physics
\bibliography{responses1} 


\end{document}
