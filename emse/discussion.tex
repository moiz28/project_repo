
In the previous section, we find that there is a discrepancy between performance testing results from the virtual and physical environments. However, such discrepancy can also be due to other factors such 1) the instability of the virtual environments, 2) the virtual machine that we used or 3) the different hardware resources on the virtual environments. Therefore, in this section, we examine the impact of such factors to better understand our results. 


\subsection{Instability of virtual environment}

If the performance of virtual machines are unstable, the observed discrepancy in Section~\ref{sec:results} may due to the instability of virtual environment, i.e., the performance testing results are not repeatable from virtual environments. In order to study whether virtual environment is unstable, leading to discrepancy in performance testing results, we repeat the same performance tests on the virtual environments for both subject systems. We perform the data analysis in Section~\ref{sec:model} by building statistical models using performance metrics. %Table~\ref{tab:stabilityvm} shows the median absolute percentage error from building a model using one virtual environment and testing on another virtual environment. 
We find that external validation error (0.04 and 0.13 for CloudStore and DS2) is almost as low as the internal validation error (0.03 and 0.09 for CloudStore and DS2). Such low error shows that the performance testing results from the virtual environments are rather stable. 

\subsection{Virtual machine software for the virtual environment}

We also investigated the impact of choosing different virtual machine software on our experimental results. We set up another virtual environment using VMWare (version 12) with the same allocated computing resources as when we set up Virtual Box. We repeat the performance tests for both subject systems. We train statistical models on the performance testing results from VMWare and test on the results from both the original virtual environment data (Virtual Box) and the results from the physical environments. We could not apply the normalization by deviance for the data from VMWare since some of the significant metrics in the model have a median absolute deviance of 0, making the normalized metric value to be infinite (see Equation~\ref{equ:mad}). We only apply the normalization by load. The low error on Virtual Box in Table~\ref{tab:vmware} shows that the performance testing results from the two different virtual machine software is similar. In addition, the high error when predicting with physical environment agrees with the results when testing with the performance testing results from the Virtual Box (see Table~\ref{tab:errors}). Such results show that the discrepancy  observed during our experiment also exits with the virtual environments that are set up with VMWare.

\begin{table}[tbh]
	\centering
	\caption{Median absolute percentage error from building a model using VMWare data.}
	\label{tab:vmware}
	%\resizebox{\columnwidth}{!}{%
		\begin{tabular}{|c||c|c|}
			\hline
			\multirow{2}{*}{\textbf{Validation type}} & \multicolumn{2}{c|}{\textbf{Median absolute percentage error}} \\ \cline{2-3} 
			& \textbf{CloudStore} & \textbf{DS2} \\ %\hline
			\midrule
			External validation with Virtual Box results& 0.07&0.10\\ \hline
%			External validation with physical normalization by deviance & 0.07 &0.06 \\ \hline
			External validation with physical normalization by load & 7.52& 1.63 \\ \hline
		\end{tabular}
	%	%
	%}
\end{table}

\subsection{Resource Allocation}

We study the impact of changing the allocated resources on the results of our analysis. We only increase the allocated resources in order to ensure the execution of the subject systems. We increase the computing resources allocated to the virtual environments by increasing the CPU to be 3 cores and increasing the memory to be 5GB. We cannot allocate more resource to the virtual environment since we need to keep resources for the hosting OS. Similarly, we train statistical models on the new performance testing results and tested it on the performance testing results from the physical environment. Similar to the results shown in Table~\ref{tab:errors}, the prediction error is high when normalizing based on load (1.57 for DS2 and 1.25 for CloudStore), while normalizing based on deviance can significantly reduce the error (0.09 for DS2 and 0.07 for CloudStore). Such results show the minimal impact to our findings by changing resource allocation. Moreover, our results demonstrate the ability of reducing discrepancy in performance testing results by using normalization based on deviance. 
