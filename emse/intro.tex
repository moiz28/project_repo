%Cloud computing has eliminated the need for hardware decisions. It is now a significant part of the IT industry incorporating not only the leading names like \textit{Facebook, Amazon, Google} etc but also small-scaled business ventures that are migrating on to a cloud based environment. According to a survey \textit{'survey'}, up to 70\% of the companies are moving from conventional physical data servers? to cloud based servers. The primary concerns when migrating are, usually, costs, availability performance etc of the system. 

%Software Performance Engineering(SPE) incorporates all the software engineering activities carried out during the project's lifecycle required to meet the software's performance requirements.
%These performance requirements ensure that the user requests are served are in a timely manner and the software's response time does not degrade with the increase in workload i.e. users. In the midst of todays large-scale software systems, (e.g.Amazon and Google's Gmail), SPE plays a vital role as most of the failures are associated with performance than with feature bugs. A second's downtime of amazon.com would cost millions of dollars. One of the recent examples in this regard was the roll-out failure of healthcare.gov. Another example was the crash of Facebook which caused NASDAQ a high monetary misfortune. 

%To avoid performance failures, software performance engineers use performance testing. The tests are used to exercise the subject system which is under test. To exercise this system, it is subjected to different workloads. These tests are designed to uncover performance bottlenecks or a test objective like maximum operational capacity. 

%The goal of performance testing is to test how the system responds are realistic workloads. Therefore performance test are composted of test workload and configuration of SUT which represents a field like environment. 

%The goal of this study is to compare the performance of a SUT in heterogeneous environments. Due to lack of resources and constant requirement of evolution of the testing environment practitioners often rely on testing done in virtual environments. However, the road that leads to the reliability of performance activities done in virtual environments still remains undiscovered. 

%Performance is a big deal
Software performance assurance activities play a vital role in the development of large software systems. These activities ensure that the software meets the desired performance requirements~\cite{futureofspe}. All too often, failures in large software systems are due to performance issues rather than functional bugs~\cite{tailatscale, foo2010mining}. Such failures lead to eventual decline in the quality of the system with reputational issues and monetary losses~\cite{costofdowntime}. For instance, Amazon estimates that a one-second page-load slowdown can cost up to \$1.6 billion~\cite{amazononesec}. 

%The impact from performance issues on system reliability would also lead to serious reputational issues.

%an interruption in the Amazon Web Service lead to the disruption of Quora, Reddit, Foursquare and numerous other web sites~\cite{amazondown}. 

%In the software ecosystem, performance assurance activities play a vital role \cite{Shang:2015:ADP:2668930.2688052}. In essence, these activities ensure a consistent software functionality. The trend of dedicating a large chunk of costs, in some cases even exceeding the cost of development \cite{bertolino2007software} to such performance assurance activities is now not unusual. In fact, most of the problems in the field are due to performance related issues \cite{foo2010mining} . A failure here would not only include an eventual decline in the quality of the software but also monetary and temporal losses. That is why companies like \textit{Facebook, Amazon and Google} are committed to achieve excellence in this regard. \cite{jackson2010performance}

% Practitioners use performance tests
In order to mitigate performance issues and ensure software reliability, practitioners often conduct performance tests~\cite{futureofspe}. Performance tests apply a workload (e.g., mimicking users' behavior in the field) on the software system~\cite{ranjanbook,Syer2016}, and monitor performance metrics, such as CPU usage, that are generated from the tests. Practitioners use such metrics to gauge the performance of the software system and identify potential performance issues (such as memory leaks~\cite{markicsm2013} and throughput bottlenecks~\cite{5635038}).

 %Performance tests are subjected to highlight system's performance which are congruent to a field-like load~\cite{Shang:2015:ADP:2668930.2688052, Syer2016}. For example, to investigate the performance bottlenecks, the maximum throughput of the system~\cite{syer2014maintenance} or other the non-functional performance requirements.

%The objective behind performance regression testing is to identify if there exists a lapse in performance for the newer version of the software compared to the previous versions. The system is tested by applying a fixed load which is congruent to a field-like load \cite{Shang:2015:ADP:2668930.2688052} \cite{foo2010mining} \cite{5306331}. The performance analysts then look for deviations between metrics values compared to the earlier versions. Examples of factors causing performance lapse may be because of high CPU utilization or a memory leak. \cite{5306331}. As there are no benchmarks for measuring the software performance cross environments, and with little or no time dedicated to performance assurance activities practitioners often find it hard to test and analyze the results of regression testing.

%The need for performance testing environments to advance and evolve is continually augmenting and so is the cost associated with the environment~\cite{stpmag, bertolino2007software}. 

%One of challenges practitioners face is the lack of available resources for performance testing. For instance

%People perform things on virtual environments
Since performance tests are often performed on large-scale software systems, the performance tests often require many resources. Moreover, performance tests often need to run for a long period of time in order to build statistical confidence on the results~\cite{ranjanbook}. In addition, such testing environment needs to be easily configurable such that a specific environment can be mimicked, reducing false performance issues. For example, issues that are related to the environment. Therefore, to address such challenges, virtual environments (i.e., VMs) are often leveraged for performance testing~\cite{whyvirtualisbetter, vmwarehighcost, whyvirtualisbetter}. The flexibility of virtual environments enables practitioners to easily prepare, customize, use and update performance testing environments in an efficient manner.

%Making it worse, the diversified and ever-changing users' behaviour forces the testing environments to be frequently customized and updated~\cite{Syer2016}.

%More importantly, performance testing is often last stage of the software development lifecycle which forces the managers to dedicate a minimal time for performance testing which can even span out to days. That's why practitioners prefer testing the system in a virtual environment~\cite{whyvirtualisbetter, vmwarehighcost}. The choice of running the performance assurance activities in a virtual environment is also based on the complexity of the large scale software systems. This enforces a virtual set up of the environment which saves resources and is easier to set up according to the desired needs~\cite{VMWarePowerCLIBlog, seetharaman2006test}. 

% No one looked at the applicability across platforms
%However, a major question that lingers is that \textbf{are performance tests executed in a virtual environment representative of what happens in the physical environment?}. This question is particularly important since 1) virtual environments are highly leveraged in practice~\cite{Nguyen:2012:ADP:2188286.2188344,xiong2013vperfguard} and 2) prior work has shown that using virtual machines imposes a hidden overhead that is rarely considered~\cite{menon2005diagnosing}, impacting the reliability of performance test results performed in virtual environments. 

Prior studies show that virtual environments are widely exploited in practice~\cite{Cito:2015:MCA:2786805.2786826,Nguyen:2012:ADP:2188286.2188344,xiong2013vperfguard}. Studies have investigated the overhead that is associated with virtual environments~\cite{menon2005diagnosing}. Such overheads may not impose effect on the results of performance tests carried out in physical and virtual environments. For example, if the performance (e.g., throughput) of the system follows the same trend in physical and virtual environments, such overhead would not significantly impact on the practitioners who examine the perform testing results. To the best of our knowledge, the discrepancy between performance testing results in virtual and physical environments has never been studied. Exploring, identifying and minimizing such discrepancy would help practitioners and researchers understand and leverage performance testing results from virtual and physical environments.




%Whether virtual environments are applicable in performance assurance activities or if they can be relied to behave equivalent to the physical servers still remains questionable. There have been limited instances of diagnosis of performance overheads~\cite{menon2005diagnosing} in the domain of performance testing however no concrete conclusions have been drawn yet. 

%The goal of our study is evaluating the discrepancy between the performance testing results from virtual and physical environments. 
%Additionally, we investigated if the performane tests done in virtual environments are repeatable or not. 
%Additionally, if they do not belong to the same population, then how impactful and effective are the prevalent discrepancies for a model-based regression testing approach.    


%what we do
In this paper, we perform a study on two open-source systems, DS2~\cite{delldvd} and CloudStore~\cite{cloudstore}, where performance tests are conducted on virtual and physical environments. Our study focuses on the discrepancy between the two environments, the impact of discrepancy on performance testing results and highlights potential opportunities to minimize the discrepancy. In particular, we compare performance testing results from virtual and physical environments based on the three widely examined aspects, i.e., individual performance metric, the relationship between the performance metrics and models that predict performance. 
%by running performance tests in both virtual and physical environments. We compare the performance test results that are generated from both the environments. In particular, we compare the performance testing results by 1) examining individual performance metric, 2) examining relationship among performance metrics and 3) building statistical models using performance metrics. 

%what we find
We find that 1) performance metrics have different shapes of distributions and trends in virtual environments compared to physical environments, 2) there are large differences in correlations among performance metrics measured in virtual and physical environments, and 3) statistical models using performance metrics from virtual environments do not apply to physical environments (i.e., produce high prediction error) and vice versa. Then, we examined the feasibility of using normalizations to help alleviate the discrepancy between performance metrics. We find that in some cases, normalizing performance metrics based on deviance may reduce the prediction error when using performance metrics collected from one environment and applying it on another. Our findings show that practitioners cannot assume that their performance tests that are observed on one environment will necessarily apply to another environment. The overhead from virtual environment does not only impact the scale of the performance metrics, but also impacts the relationship among performance metrics. On the other hand, we find that practitioners who leverage both, virtual and physical environments, may be able to reduce the discrepancy that may arise due to the environment (i.e., virtual vs. physical) by applying normalization techniques.

%Our findings highlight the need of awareness of the discrepancy between performance testing results in virtual and physical environments, and the need to research efforts on investigating how to improve the use of both virtual and physical environments to ensure system reliability.

%We leverage a heatmap to visualize the changes in correlations among performance metrics and the system load metric.

% cannot apply on the performance metrics collected from another environments (with high prediction error), even after normalizing the performance metrics.


%In particular, we first compare the performance metrics values and distributions. We then investigate the correlation of performance counters to the generated load. Finally, we build statistical models to help us reach conclusions. 
%We observed that majority of the performance counters do not belong to the same family of distribution. We also observed that the metric that are highly correlated with the load are not the same across both our subject systems.
%We concluded that due to the discrepancies present in the virtual environment, we can not rely on performance evaluated in the virtual environment as is. We then apply scaling techniques to the metrics generated in the virtual environment to build linear regression models. We concluded that the performance metrics in the virtual environment are not identical copies of the metrics in the physical environment. 


% to determine and compare the distributions. This is achieved by comparing plots and finding correlation between the metrics cross-environments. We use generalized linear regression models, as used in the performance assurance activities \cite{Shang:2015:ADP:2668930.2688052}, to determine the extent of the comparison between metrics. If the metrics are transferable between the environments, we should expect to see a low percentage error. We chose to build our regression models based on multiple performance metrics to see the effect of the clustered performance metrics. 

%We diffuse our findings by answering the following research questions:

%\begin{description}
%	\item[$\bullet$] RQ1: Do performance metrics from physical and virtual environments belong to the same population?
%	\item[$\bullet$] RQ2: Is the metric correlation with load same cross-environments?
%	\item[$\bullet$] RQ3: Do performance metrics from different environments impact performance modeling?
	
%\end{description}

The rest of the paper is organized as follows. Section~\ref{sec:related} presents the background and related work to this paper. Section~\ref{sec:case} presents the case study step. Section~\ref{sec:results} presents the results of our cases study, followed by a discussion of our results in Section~\ref{sec:discussion}. Section~\ref{sec:threats} discusses the threats to validity of our findings. Finally, Section~\ref{sec:conclusion} concludes this paper.
