% -*- root: cuthesis_masters.tex -*-  

%At the core of any software system is the software team who develop it...

\section{Introduction}
Software performance assurance activities play a vital role in the development of large software systems. These activities ensure that the software meets the desired performance requirements~\cite{futureofspe}. Often however, failures in large software systems are due to performance issues rather than functional bugs~\cite{tailatscale, foo2010mining}. Such failures lead to the eventual decline in quality of the system with reputational and monetary consequences~\cite{costofdowntime}. For instance, Amazon estimates that a one-second page-load slowdown can cost up to \$1.6 billion~\cite{amazononesec}. 

In order to mitigate performance issues and ensure software reliability, practitioners often conduct performance tests~\cite{futureofspe}. Performance tests apply a workload (e.g., mimicking users' behavior in the field) on the software system~\cite{ranjanbook,Syer2016}, and monitor performance metrics, such as CPU usage, that are generated based on the tests. Practitioners use such metrics to gauge the performance of the software system and identify potential performance issues (such as memory leaks~\cite{markicsm2013} and throughput bottlenecks~\cite{5635038}).

Since performance tests are often performed on large-scale software systems, the performance tests often require many resources~\cite{ranjanbook}. Moreover, performance tests often need to run for a long period of time in order to build statistical confidence on the results~\cite{ranjanbook}. Such testing environments need to be easily configurable such that a specific environment can be mimicked, reducing false performance issues. For example, issues that are related to the environment. Hence, due to their flexibility, virtual environments enable practitioners to easily prepare, customize, use and update performance testing environments in an efficient manner. Therefore, to address such challenges, virtual environments (VMs) are often leveraged for performance testing~\cite{whyvirtualisbetter, vmwarehighcost}. The use of VMs in performance testing are widely discussed~\cite{performanceonvvirtual, stackoverflow, windowsserver}, and even well documented~\cite{sugarcrmexp} by practitioners. In addition, many software systems are released both on-premise (physical) and on cloud (virtual) environment (e.g., SugarCRM~\cite{sugarcrm} and BlackBerry Enterprise Server~\cite{bes}). Hence, it is important to conduct performance testing on both the virtual (for cloud deployment) and physical environments (for on-premise deployment).

\begin{sloppypar}
Prior studies show that virtual environments are widely exploited in practice~\cite{Cito:2015:MCA:2786805.2786826,Nguyen:2012:ADP:2188286.2188344,xiong2013vperfguard}. Studies have investigated the overhead that is associated with virtual environments~\cite{menon2005diagnosing}. Such overheads may not impose effect on the results of performance tests carried out in physical and virtual environments. For example, if the performance (e.g., throughput) of the system follows the same trend (or distribution) in both, the physical and virtual environments, such overhead would not significantly impact the outcome for the practitioners who examine the performance testing results. \textit{Our work is one of the first works that examine such discrepancy between performance testing results in virtual and physical environments}. Exploring, identifying and minimizing such discrepancy will help practitioners and researchers understand and leverage performance testing results from virtual and physical environments. Without knowing if there exists a discrepancy between the performance testing results from the two environments practitioners cannot rely on the performance assurance activities carried out in the virtual environment or vice versa. Once the discrepancy is identified, the performance results could be evaluated more accurately.
\end{sloppypar}

%what we do
In this thesis, we perform a study on two open-source systems, DS2~\cite{delldvd} and CloudStore~\cite{cloudstore}, where performance tests are conducted on virtual and physical environments. Our study focuses on the discrepancy between the two environments, the impact of discrepancy on performance testing results and highlights potential opportunities to minimize the discrepancy. In particular, we compare performance testing results from virtual and physical environments based on the three widely examined aspects, i.e., individual performance metric, the relationship between the performance metrics and models that predict performance. 

\begin{itemize}
	\item \textit{single performance metric}: the trends and distributions of each performance metric
	\item \textit{the relationship between the performance metrics}: the correlations between every two performance metrics 
	\item \textit{statistical performance models}: the models that are built using performance metrics to predict the overall performance of the system
	
\end{itemize}
%by running performance tests in both virtual and physical environments. We compare the performance test results that are generated from both the environments. In particular, we compare the performance testing results by 1) examining individual performance metric, 2) examining relationship among performance metrics and 3) building statistical models using performance metrics. 

%what we find


%Our findings highlight the need of awareness of the discrepancy between performance testing results in virtual and physical environments, and the need to research efforts on investigating how to improve the use of both virtual and physical environments to ensure system reliability.

%We leverage a heatmap to visualize the changes in correlations among performance metrics and the system load metric.

% cannot apply on the performance metrics collected from another environments (with high prediction error), even after normalizing the performance metrics.


%In particular, we first compare the performance metrics values and distributions. We then investigate the correlation of performance counters to the generated load. Finally, we build statistical models to help us reach conclusions. 
%We observed that majority of the performance counters do not belong to the same family of distribution. We also observed that the metric that are highly correlated with the load are not the same across both our subject systems.
%We concluded that due to the discrepancies present in the virtual environment, we can not rely on performance evaluated in the virtual environment as is. We then apply scaling techniques to the metrics generated in the virtual environment to build linear regression models. We concluded that the performance metrics in the virtual environment are not identical copies of the metrics in the physical environment. 


% to determine and compare the distributions. This is achieved by comparing plots and finding correlation between the metrics cross-environments. We use generalized linear regression models, as used in the performance assurance activities \cite{Shang:2015:ADP:2668930.2688052}, to determine the extent of the comparison between metrics. If the metrics are transferable between the environments, we should expect to see a low percentage error. We chose to build our regression models based on multiple performance metrics to see the effect of the clustered performance metrics. 

%We diffuse our findings by answering the following research questions:

%\begin{description}
%	\item[$\bullet$] RQ1: Do performance metrics from physical and virtual environments belong to the same population?
%	\item[$\bullet$] RQ2: Is the metric correlation with load same cross-environments?
%	\item[$\bullet$] RQ3: Do performance metrics from different environments impact performance modeling?

%\end{description}

\section{Research Hypothesis}
Prior research and our industrial experience lead us to an investigation based on the following hypothesis:
%	\fbox{%
%		\parbox{\textwidth}{%
%			\textit{
%			For large-scale software systems, performance assurance activities are carried primarily in virtual environments. There is little research regarding the reliability of testing in a different environment compared to physical. We hypothesize that for software testing activities there exists a discrepancy between physical and virtual environments. We believe that the approaches used so far do not scale well. 
%			Furthermore, we believe practitioners should be aware of such existing discrepancies when analyzing software performance in a foreign environment.
%				   }
%		}%
%	}
	
\begin{framed}
		\textit{For large-scale software systems, performance assurance activities are carried primarily in virtual environments. There is little research regarding the reliability of testing in a different environment compared to physical. We hypothesize that for software testing activities there exists a discrepancy between physical and virtual environments. We believe that the approaches used so far do not scale well.} 
		\par 	
		\textit{Furthermore, we believe practitioners should be aware of such existing discrepancies when analyzing software performance in a foreign environment.}
\end{framed}



\section{Thesis Overview}

%The rest of the paper is organized as follows. Section~\ref{sec:related} presents the background and related work. Section~\ref{sec:case} presents the case study setup. Section~\ref{sec:results} presents the results of our cases study, followed by a discussion of our results in Section~\ref{sec:discussion}. Section~\ref{sec:threats} discusses the threats to validity of our findings. Finally, Section~\ref{sec:conclusion} concludes this paper.

\textbf{Chapter 2: Literature Review:} In this chapter, we discuss the related work that has discussed performance assurance activities carried out in virtual environments and the associated overhead caused by it. The chapter is divided in two parts:

	\textbf{Part I:} Addresses the methodologies that are used to detect performance anomalies. We present the work sub divided into three categories: individual performance metrics, relationship among performance metrics, statistical modeling based on performance metrics.   
	
    \textbf{Part II:} Addresses the work that points to the overheads present in virtual environments. We present the state-of-the-art approaches used to detect performance anomalies and how such approaches helps us in detection of overheads. The chapter wraps up by comparing the current limitations and our work in the field of performance engineering.

From the literature review, we deduced that a lot of work has been affiliated to the generation of performance alarms and the detection of performance issues however little work has highlighted the reliability of testing done in virtual environments.
\\

\noindent\textbf{Chapter 3: Testing For the Ubiety of Discrepancy:} In this chapter, we perform case studies on two open source subject systems. We generate the performance metrics by applying realistic and varying workloads in identically set up physical and virtual environments. Followed up data cleansing and removing any reasonable outliers present in our data. Next, we analyze the performance metrics in three possible ways: individually, as a collection, and as an input to statistical models. 
\\

\noindent\textbf{Chapter 4: Studying the impact of modifying the virtual environment}
Furthermore, we elaborate and solidify the conclusion drawn by discussing possibilities as how the use of virtual environments may effect our results: by changing the type of the virtual machine, by modifying the allocated resources and testing the repeatability(hence the stability) of our chosen virtual environment.
Lastly, we discuss the threats to validity for this thesis.
\\

\noindent\textbf{Chapter 5: Summary, Contributions and Future Work}
We conclude the thesis by summarizing our work. We also highlight how our findings can be leveraged by the practitioners, serving as the stepping stone towards future work.  
\\


%\noindent\textbf{Chapter 6: Appendix}
\section{Thesis Contributions}

The major contributions of thesis are as follows:

\begin{itemize}
	\item An extensive review of the state of the art in software performance activities carried out in virtual environments. Such a review is necessary for the practitioners to be aware of the hiccups associated to software performance in virtual environments.
	
	\item We conclude performance metrics have different shapes of distributions and trends in virtual environments compared to physical environments. There are large differences in correlations among performance metrics measured in virtual and physical environments.
	\item We highlight statistical models using performance metrics from virtual environments do not apply to physical environments (i.e., produce high prediction error) and vice versa.
	\item  We examined the feasibility of using normalizations to help alleviate the discrepancy between performance metrics. We find that in some cases, normalizing performance metrics based on deviance may reduce the prediction error when using performance metrics collected from one environment and applying it on another. 
	\item We demonstrate that practitioners cannot assume that their performance tests that are observed on one environment will necessarily apply to another environment. The overhead from virtual environments does not only impact the scale of the performance metrics, but also impacts the relationship among performance metrics. On the other hand, we find that practitioners who leverage both, virtual and physical environments, may be able to reduce the discrepancy that may arise due to the environment (i.e., virtual vs. physical) by applying normalization techniques.
\end{itemize}