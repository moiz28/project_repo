% -*- root: cuthesis_masters.tex -*-  

%At the core of any software system is the software team who develop it...

\section{Introduction}
Software performance assurance activities play a vital role in the development of large software systems. These activities ensure that the software meets the desired performance requirements~\cite{futureofspe}. Often however, failures in large software systems are due to performance issues rather than functional bugs~\cite{tailatscale, foo2010mining}. Such failures lead to the eventual decline in quality of the system with reputational and monetary consequences~\cite{costofdowntime}. For instance, Amazon estimates that a one-second page-load slowdown can cost up to \$1.6 billion~\cite{amazononesec}. 

In order to mitigate performance issues and ensure software reliability, practitioners often conduct performance tests~\cite{futureofspe}. Performance tests apply a workload (e.g., mimicking users' behavior in the field) on the software system~\cite{ranjanbook,Syer2016}, and monitor performance metrics, such as CPU usage, that are generated based on the tests. Practitioners use such metrics to gauge the performance of the software system and identify potential performance issues (such as memory leaks~\cite{markicsm2013} and throughput bottlenecks~\cite{5635038}).

Since performance tests are often performed on large-scale software systems, the performance tests often require many resources~\cite{ranjanbook}. Moreover, performance tests often need to run for a long period of time in order to build statistical confidence on the results~\cite{ranjanbook}. Such testing environments need to be easily configurable such that a specific environment can be mimicked, reducing false performance issues, e.g. issues that are related to the environment. Hence, due to their flexibility, virtual environments (VMs) enable practitioners to easily prepare, customize, use and update performance testing environments in an efficient manner. Therefore, to address such challenges, virtual environments are often leveraged for performance testing~\cite{whyvirtualisbetter, vmwarehighcost}. The use of VMs in performance testing are widely discussed~\cite{performanceonvvirtual, stackoverflow, windowsserver}, and even well documented~\cite{sugarcrmexp} by practitioners. In addition, many software systems are released both on-premise (physical) and on cloud (virtual) environment (e.g., SugarCRM~\cite{sugarcrm} and BlackBerry Enterprise Server~\cite{bes}). Hence, it is important to conduct performance testing on both the virtual (for cloud deployment) and physical environments (for on-premise deployment).

\begin{sloppypar}
Prior studies show that virtual environments are widely exploited in practice~\cite{Cito:2015:MCA:2786805.2786826,Nguyen:2012:ADP:2188286.2188344,xiong2013vperfguard}. Studies have investigated the overhead that is associated with virtual environments~\cite{menon2005diagnosing} and concluded that the computational overhead may effect the system performance in a virtual environment. Such overheads may not impose effect on the results of performance tests carried out in physical and virtual environments. For example, if the performance (e.g., throughput) of the system follows the same trend (or distribution) in both, the physical and virtual environments, such overhead would not significantly impact the outcome for the practitioners who examine the performance testing results. \textit{Our work is one of the first works that examine such discrepancy between performance testing results in virtual and physical environments}. Exploring, identifying and minimizing such discrepancy will help practitioners and researchers understand and leverage performance testing results from virtual and physical environments. Without knowing if there exists a discrepancy between the performance testing results from the two environments practitioners cannot rely on the performance assurance activities carried out in the virtual environment or vice versa. Once the discrepancy is identified, the performance results could be evaluated more accurately.
\end{sloppypar}

%what we do
In this thesis, we empirically demonstrate the evidence of discrepancy present between the two environments with the data generated by our performance testing results. We perform a study on two open-source systems, Dell DVD Store (DS2)~\cite{delldvd} and CloudStore~\cite{cloudstore}, and the performance tests are conducted on virtual and physical environments. Our study focuses on the discrepancy between the two environments, the impact of discrepancy on performance testing results and highlights potential opportunities to minimize the discrepancy. In particular, we compare performance testing results from virtual and physical environments based on the three widely examined aspects, i.e., individual performance metric, the relationship between the performance metrics and models that predict performance. 

\begin{itemize}
	\item \textit{single performance metric}: the trends and distributions of each performance metric. Such analysis is to identify the trends and shape of the distributions of performance metrics.
	Due to the difference between testing environments, performance testing results are expected to be
	different in raw value. However, the shape of distribution and the trend should be similar. We investigate such distributions and trends. 
	\item \textit{the relationship between the performance metrics}: the correlations between every two performance metrics. Combinations of performance metrics are significantly more predictive
	towards performance issues than individual metrics. We believe that a change in these combinations
	of relationships can reflect the discrepancy of performance in the two environments.
	\item \textit{statistical performance models}: the models that are built using performance metrics to predict the overall performance of the system. Our third type of analysis is used to see the combination of all performance metrics all together by
	constructing a statistical model.
	
\end{itemize}
%by running performance tests in both virtual and physical environments. We compare the performance test results that are generated from both the environments. In particular, we compare the performance testing results by 1) examining individual performance metric, 2) examining relationship among performance metrics and 3) building statistical models using performance metrics. 

%what we find


%Our findings highlight the need of awareness of the discrepancy between performance testing results in virtual and physical environments, and the need to research efforts on investigating how to improve the use of both virtual and physical environments to ensure system reliability.

%We leverage a heatmap to visualize the changes in correlations among performance metrics and the system load metric.

% cannot apply on the performance metrics collected from another environments (with high prediction error), even after normalizing the performance metrics.


%In particular, we first compare the performance metrics values and distributions. We then investigate the correlation of performance counters to the generated load. Finally, we build statistical models to help us reach conclusions. 
%We observed that majority of the performance counters do not belong to the same family of distribution. We also observed that the metric that are highly correlated with the load are not the same across both our subject systems.
%We concluded that due to the discrepancies present in the virtual environment, we can not rely on performance evaluated in the virtual environment as is. We then apply scaling techniques to the metrics generated in the virtual environment to build linear regression models. We concluded that the performance metrics in the virtual environment are not identical copies of the metrics in the physical environment. 


% to determine and compare the distributions. This is achieved by comparing plots and finding correlation between the metrics cross-environments. We use generalized linear regression models, as used in the performance assurance activities \cite{Shang:2015:ADP:2668930.2688052}, to determine the extent of the comparison between metrics. If the metrics are transferable between the environments, we should expect to see a low percentage error. We chose to build our regression models based on multiple performance metrics to see the effect of the clustered performance metrics. 

%We diffuse our findings by answering the following research questions:

%\begin{description}
%	\item[$\bullet$] RQ1: Do performance metrics from physical and virtual environments belong to the same population?
%	\item[$\bullet$] RQ2: Is the metric correlation with load same cross-environments?
%	\item[$\bullet$] RQ3: Do performance metrics from different environments impact performance modeling?

%\end{description}

\section{Research Hypothesis}
Prior research and our industrial experience lead us to an investigation based on the following hypothesis:
%	\fbox{%
%		\parbox{\textwidth}{%
%			\textit{
%			For large-scale software systems, performance assurance activities are carried primarily in virtual environments. There is little research regarding the reliability of testing in a different environment compared to physical. We hypothesize that for software testing activities there exists a discrepancy between physical and virtual environments. We believe that the approaches used so far do not scale well. 
%			Furthermore, we believe practitioners should be aware of such existing discrepancies when analyzing software performance in a foreign environment.
%				   }
%		}%
%	}
	
\begin{framed}
		\textit{For large-scale software systems, performance assurance activities are carried primarily in virtual environments. There is little research on the presence of performance discrepancy in a virtual environment compared to physical. We hypothesize that for software testing activities there exists a discrepancy between physical and virtual environments.} %We believe that the approaches used so far do not scale well.} 
		\par 	
		\textit{Furthermore, we believe that the practitioners should be aware and reduce such existing discrepancies when analyzing software performance in a foreign environment.}
\end{framed}



\section{Thesis Overview}

%The rest of the paper is organized as follows. Section~\ref{sec:related} presents the background and related work. Section~\ref{sec:case} presents the case study setup. Section~\ref{sec:results} presents the results of our cases study, followed by a discussion of our results in Section~\ref{sec:discussion}. Section~\ref{sec:threats} discusses the threats to validity of our findings. Finally, Section~\ref{sec:conclusion} concludes this paper.

\textbf{Chapter 2: Literature Review:} In this chapter, we discuss the background of performance assurance activities. We also discuss performance testing carried out in virtual environments and the associated overhead caused by it. The chapter is divided in two parts:

	\textbf{Part I:} Discusses the role of software performance testing and how a software performance test is carried out, from setting up the system to the point of data collection and analysis. Furthermore, this part also addresses the differences and similarities between software performance testing, load testing and stress testing.

	\textbf{Part II:} Addresses the methodologies that are used to analyze performance testing results. We present the work sub divided into three categories: single performance metric, relationship among performance metrics, statistical modeling based on performance metrics. This part also addresses the work that points to the overheads present in virtual environments. We present the state-of-the-art approaches used to analyze performance testing results and how such approaches help us in detection of overheads. Additionally, we also discuss the role of performance testing in bug detection. 
	%The chapter wraps up by comparing the current limitations and our work in the field of performance engineering.

From the literature review, we deduced that much prior work has been affiliated to the generation of performance alarms and the detection of performance issues however little work has highlighted the testing done in virtual environments.
\\

\noindent\textbf{Chapter 3: Testing For The Presence Of Discrepancy:} In this chapter, we perform case studies on two open source subject systems. We generate the performance metrics by applying realistic and varying workloads in identically set up physical and virtual environments. Next, it is followed up by data cleansing. Finally, we analyze the performance metrics in three possible ways: individually, as a collection, and as an input to statistical models. 
\\

\noindent\textbf{Chapter 4: Discussion On The Impact From Other Factors:}
Furthermore, we elaborate and solidify the conclusion drawn by discussing possibilities on how the use of virtual environments may affect our results: by changing the type of the virtual machine, by modifying the allocated resources and testing the repeatability(hence the stability) of our chosen virtual environment.
Lastly, we discuss the threats to validity for this thesis.
\\

\noindent\textbf{Chapter 5: Summary, Contributions And Future Work:}
We conclude the thesis by summarizing our work. We also highlight how our findings can be leveraged by the practitioners, serving as the stepping stone towards future work.  
\\


%\noindent\textbf{Chapter 6: Appendix}

\section{Thesis Contributions}

The major contributions of thesis are as follows:

\begin{itemize}
	\item An extensive review of the state of the art analysis in software performance activities. Such a review is necessary for the practitioners to be aware of the discrepancies associated to software performance across different environments.
	
	\item We find the performance metrics have different shapes of distributions and trends in virtual environments compared to physical environments. There are large differences in correlations among performance metrics measured in virtual and physical environments.
	\item We highlight statistical models using performance metrics from virtual environments do not apply to physical environments (i.e., produce high prediction error) and vice versa.
	\item  We examined the feasibility of using normalization techniques to help alleviate the discrepancy between performance metrics. We find that in some cases, normalizing performance metrics based on deviance may reduce the prediction error when using performance metrics collected from one environment and applying it on another. 
	%\item We demonstrate that practitioners cannot assume that their performance tests that are observed on one environment will necessarily apply to another environment. The overhead from virtual environments does not only impact the scale of the performance metrics, but also impacts the relationship among performance metrics. On the other hand, we find that practitioners who leverage both, virtual and physical environments, may be able to reduce the discrepancy that may arise due to the environment (i.e., virtual vs. physical) by applying normalization techniques.
\end{itemize}