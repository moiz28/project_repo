% -*- root: cuthesis_masters.tex -*-

%In this chapter we present related work on ...

Software systems are expected to serve millions of concurrent requests. However, the systems are first tested to ensure that they are working correctly under a certain load(s). This load is also known as the rate at which the system is processing the requests. 
In this chapter, we discuss the motivation behind load testing and similar work done in the field of performance engineering. We later survey the state-of-the-art literature that is related to out work.

\section{Background}

Generically, performance assurance activities are carried out by analyzing the system responses on a variety of loads. For example, to detect performance regression, the system is analyzed after applying a load profile. This load profile depicts the normal workload of the system once it is functional in the field \cite{464549}.

We observe that the terms performance testing \cite{Dillenseger2009,Menasce02loadtesting,Menasce:2002}, load testing \cite{536461,Bayan:2008,perf_load_stress,perf_web} and stress testing \cite{Bayan:2008,Yang:1996,4020172} are also used interchangeably. 

\subsection{What is the difference between load testing, stress testing and performance testing?}

Although there are similarities between with these three types of testing techniques, for example all of these are carried out after functional testing, we now differentiate between the application of these tests.

\subsubsection{Load Testing}

The rate at which which the requests are submitted to a system is called the load \cite{Beizer:1984}. This system is more commonly known as system under test or \textit{SUT}. Load testing is carried out later than conventional testing in a software's life. This may be done on a prototype or a working system than a design or a model. Load testing is used to detect load related problems. For example, deadlocks, buffer overflows, high response times and low throughput \cite{464549,Barna:2011,6032540}. 

In some exceptional cases where the non-functional requirements are not present, one of ways to determine if the test has passed is by comparing it to the results of the previous version. This is also known as the "no-worse-than-before" principle. As the name suggests, the version being tested should be, if not better, equal to the previous version. \cite{Dumke:2001}

\subsubsection{Performance Testing}

Although there may be some similarities present between performance and load testing, performance testing is mainly focused to detect primarily performance related performance with the software system. For example, response time, throughput and resource utilization. \cite{Barna:2011,6032540,Gorton}

Performance test is detailed than load testing as it may be used to cover designs \cite{csurgay1999performance,denaro2004early,denaro2005performance}, algorithms \cite{cangussu2009segment,cangussu2007reducing}, and system configurations \cite{hoskins2005software,pozin2011models,sopitkamol2005method}. The goal behind performance testing may be to test performance requirements \cite{pozin2011models} or exploratory. For example, to answer questions such as how do various configurations impact the performance of the system \cite{Menasce:2000,Menasce:1994,Menasce:2001,pozin2011models}.

\subsubsection{Stress Testing}

Stress testing is testing the application under "stress" or extreme load. This can be used to detect how resilient the system or to detect further load-related problems. For example, memory leaks and deadlocks. These "stressful" conditions for the system can be either load related(normal \cite{zhang2002automated,kalita2011investigation,chakravarty2010stress} or heavy load \cite{Dillenseger2009,kalita2011investigation,huebner2001performance}) or limited resources allocated/failures (for example, disk or database failures) \cite{acharya2009mining}. We noted that in some cases it may also be used to detect the competency of the SUT \cite{garousi2010genetic,garousi2008empirical,garousi2006traffic,garousi2008traffic}.


Having stated all of the above, there are instances where the interest of a performance test may overlap with load testing pr stress testing and vice-versa. For example, to check robustness of the system when put in extreme conditions, or to check how an algorithm works when handling large files. 

We focus on load testing, which is to detect how the SUT behaves under load to detect functional and/or non functional problems. 

\subsection{What is a load test execution?}




\section{Literature Review}

In this section, we discuss the related work of this thesis in two main areas: 1) analyzing performance metrics from performance testing and 2) analysis of VM overhead.


\subsection{Analyzing performance metrics from performance testing} 

Prior research has proposed a slew of techniques to analyze performance testing results, i.e. performance metrics. Such techniques typically examine three different aspects of the metrics: 1) individual performance metrics, 2) the relationship among performance metrics, and 3) statistical modeling based on performance metrics.


\subsection{Individual performance metrics}
\label{sec:relatedindividual}
Nguyen \textit{et al$.$}~\cite{Nguyen:2012:ADP:2188286.2188344} introduce the concept of using control charts~\cite{shewhart1931economic} in order to detect performance regressions. Control charts use a predefined threshold to detect performance anomalies. However control charts assume that the output follows a uni-model distribution, which may be an inappropriate assumption for performance. Nguyen \textit{et al$.$} propose an approach to normalize performance metrics between heterogeneous environments in order to build robust control charts. %However, the experiments are only carried out on the virtual machines in contrast to our approach.

Malik \emph{et al$.$}~\cite{Malik:2010:ACL:1955601.1955936, haroon} propose approaches that cluster performance metrics using Principal Component Analysis (PCA). Each component generated by PCA is mapped to performance metrics by a weight value. The weight value measures how much a metric contributes to the component. For every performance metric, a comparison is performed on the weight value of each component to detect performance regressions.

Heger \emph{et al$.$}~\cite{DBLP:conf/wosp/HegerHF13} present an approach that uses software development history and unit tests to diagnose the root cause of performance regressions. In the first step of their approach, they leverage Analysis of Variance (ANOVA) to compare the response time of the system to detect performance regressions. 

Similarly, Jiang \emph{et al$.$}~\cite{jackicsm2009} extract response time from system logs. Instead of conducting statistical tests, Jiang \emph{et al$.$} visualize the trend of response time during performance tests, in order to identify performance issues.


%\emad{how does this compare to our work?}
%

%The ad hoc approach may fail to detect performance regressions if the target counters do not capture the performance regressions. Moreover, such ad hoc analysis does not detect the change of relationships between counters, such as the relationship between I/O and CPU.


\subsection{Relationship among performance metrics}
\label{sec:relatedrelation}

Malik \emph{et al$.$}~\cite{5635038} leverage Spearman's rank correlation to capture the relationship among performance metrics. The deviance of correlation is calculated in order to pinpoint which subsystem should take responsibility of the performance deviation.

Foo\emph{ et al$.$}~\cite{foo2010mining} propose an approach that leverages association rules in order to address the limitations of manually detecting performance regressions in large scale software systems. Association rules capture the historical relationship among performance metrics and generate rules based on the results of prior performance tests. Deviations in the association rules are considered signs of performance regressions.

Jiang \emph{et al$.$}~\cite{5270324} use normalized mutual information as a similarity measure to cluster correlated performance metrics. Since metrics in one cluster are highly correlated, the uncertainty among metrics in the cluster should be low. Jiang \emph{et al$.$} leverage entropy from information theory to monitor the uncertainty of each cluster. A significant change in the entropy is considered as a sign of a performance fault. %During the evaluation of the approach, the authors were able to detect 77\% of the injected faults and the faulty subsystems, without having any false positives.

%Our approach also considers the correlations between performance counters by using regression models. However, our approach first aims to group performance counters into test aspects and focuses on detecting performance regressions instead of faults.

%Malik\textit{ et al.}~\cite{haroon} derive performance signatures based on both supervised and unsupervised learning to detect performance anomalies. They use Principal Component Analysis as their unsupervised learning technique if the past tests are not marked pass or fail. PCA is used to to examine the relationships between different performance metrics.

\subsection{Statistical modeling based on performance metrics}
\label{sec:relatedmodel}

Xiong \textit{et al$.$}~\cite{xiong2013vperfguard} proposed a model-driven approach named \textit{vPerfGuard} to detect software performance regressions in a cloud-environment. The approach builds models between workload metrics and a performance metric, such as CPU. The models can be used to detect workload changes and assists in identifying performance bottlenecks. Since the usage of \emph{vPerfGuard} is typically in a virtual environment, our study may help the future evaluation of \textit{vPerfGuard}. Similarly, Shang\textit{ et al.}~\cite{Shang:2015:ADP:2668930.2688052} propose an approach of including only a limited number of performance metrics for building statistical models. The approach leverages an automatic clustering technique in order to find the number of models to be build for the performance testing results. By building statistical models for each cluster, their approach is applicable to detect injected performance regressions. %We use the same technique in our study to inject performance regression in the target system nonetheless the limitation of their study to perform their experiments in a virtual environment persists.


Cohen \textit{et al$.$}~\cite{cohen2004correlating} propose an approach that builds probabilistic models, such as Tree-Augmented Bayesian Networks, to examine the causes that target the changes in the system's response time. Cohen \textit{et al$.$}~\cite{Cohen:2005:CIC:1095810.1095821} also proposed that system faults can be detected by building statistical models based on performance metrics. The approaches of Cohen \textit{et al$.$}~\cite{cohen2004correlating, Cohen:2005:CIC:1095810.1095821} were improved by Bodik \textit{et al.}~\cite{bodik2008hilighter} by using logistic regression models.

Jiang \emph{et al$.$}~\cite{Jiang:2009:SMM:1555228.1555233} propose an approach that improves the Ordinary Least Squares regression models that are built from performance metrics and use the model to detect faults in a system. The authors conclude their approach is more efficient than the current linear-model approach.

\cite{Jiang:2010:AAL:1831708.1831726} used a similar approach to model the performance testing results. The data to build statistical models is extracted from execution logs mapped to performance metrics. According to certain performance criteria, the performance models are then used to draw conclusions for bottlenecks for example indication of response time or resource usage problems. We use the same methodology to reach the conclusions in this thesis.

In our work, we compare performance testing results from both virtual and physical environments based on all the above three types of analyses. Our findings can help better evaluate and understand the findings from the aforementioned research.

%Model-based approached use the target counters(e.g. DISC I/O and CPU Utilization) to build models and these models are then used to detect performance regression int he system. What make the model-based approach celebrated is the inclusion and comparison of numerous performance counters at the same time. 
%Jiang \textit{et al. }\cite{jiang2011system} used an improved least square regression models to detect system faults.

\subsection{Analysis of VM overhead}

Kraft \textit{et al$.$}~\cite{kraft2011io} discuss the issues that are related to disk I/O in a virtual environment. They examine the performance degradation of disk request response time by recommending a trace-driven approach. Kraft \textit{et al.} emphasize on the latencies existing in virtual machine requests for disc IO due to increments in time associated with request queues. 

Aravind \textit{et al$.$}~\cite{menon2005diagnosing} audit the performance overhead in Xen virtual machines. They uncover the origins of overhead that might exist in the network I/O causing a peculiar system behavior. However, there study is limited to Xen virtual machine only while mainly focusing on network related performance overhead.

Brosig \textit{et al$.$}~\cite{brosig2013evaluating} predict the performance overhead of virtualized environments using Petri-nets in Xen server. The authors focused on the visualization overhead with respect to queuing networks only. The authors were able to accurately predict server utilization but had significant errors for multiple VMs.


Huber \textit{et al$.$}~\cite{huber2011evaluating} present a study on cloud-like environments. The authors compare the performance of virtual environments and study the degradation between the two environments. Huber \textit{et al$.$} further categorize factors that influence the overhead and use regression based models to evaluate the overhead. However, the modeling only considers CPU and memory.

%\textcolor{red}{where does the following related work fits in?}

Luo \textit{et al$.$}~\cite{Luo:2016:MPR:2901739.2901765} converge the set of inputs that may cause software regression. They apply genetic algorithms to detect such combinations. 

Prior research focused on the overhead of virtual environments without considering the impact of such overhead on performance testing and assurance activities. In this paper, we evaluate the discrepancy between virtual and physical environments by focusing on the impact of performance testing results and investigate whether such impact can be minimized in practice.

%Previous literature has its limitations as most of the performance regression testing and regression modelling  is performed in virtual environments. Through our work we validate the usage of virtual environments in the field of performance engineering.