\documentclass[12pt]{report}


%\usepackage{color}
\usepackage{url}
\usepackage{xspace}
\usepackage{cuthesis}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{csquotes}
%\usepackage{pdflscape}
\usepackage{subfigure}
%\usepackage[framemethod=TikZ]{mdframed}
\usepackage{amssymb,amsmath}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fancybox}
\usepackage[flushleft]{threeparttable}
\usepackage{framed}
\usepackage{breakcites}


\linespread{1.6}

\author{Muhammad Moiz Arif}
\title {An Empirical Study on the Discrepancy between Performance Testing Results from Virtual and Physical Environments}
\degree{Master of Applied Science in Software Engineering}
\dept  {Computer Science and Software Engineering}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% define commands for frequently used words or phrases
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newcommand{\SATD}{self-admitted technical debt\xspace}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% define commands for your RQs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\chapterIIIrqi}{\textbf{RQ:}}
\newcommand{\chapterIVrqi}{\textbf{RQ1. \\}}
\newcommand{\chapterIVrqii}{\textbf{RQ2. \\}}
\newcommand{\chapterIVrqiii}{\textbf{RQ3. \\}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% define command to create a conclusion box to answer your RQs
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\conclusionbox}[1]{%
    \vspace{6mm}
    \framebox[0.95\textwidth][c]{%
        \parbox[b]{0.92\textwidth}{%
            {\it #1}
        }
    }
    \vspace{6mm}
}


\begin{document}

\begin{abstract}

Large software systems often undergo performance tests to ensure their capability to handle expected loads. These performance tests often consume large amounts of computing resources and time since heavy loads need to be generated. Making it worse, the ever evolving field requires frequent updates to the performance testing environment. In practice, virtual machines (VMs) are widely exploited to provide flexible and less costly environments for performance tests. However, the use of VMs may introduce extra overhead (e.g., a higher than expected memory utilization) to the testing environment and lead to unrealistic performance testing results. Yet, little or no research has studied the overhead and impact on test results of using VMs in performance testing activities. 

In this dissertation we evaluate the discrepancy between the performance testing results from virtual and physical environments. We perform a case study on two open source systems - namely Dell DVD Store and CloudStore. We conduct the same performance tests in both virtual and physical environments and compare the performance testing results based on the three aspects that are typically examined for performance testing results: 1) individual performance metrics (e.g. CPU Time from virtual environment vs. CPU Time from physical environment), 2) the relationship among performance metrics (e.g. correlation between CPU and I/O) and 3) performance models that are built to predict system performance. Our results show that 1) individual performance metrics from virtual and physical environments do not follow the same distribution hence practitioners cannot simply use a scaling factor to compare the performance between environments,  2) correlations among performance metrics in virtual environments are different from those in physical environments 3) statistical models built based on the performance counters from virtual environments are different from the models built from physical environments suggesting that practitioners cannot use the performance testing results across virtual and physical environments. In order to assist the practitioners leverage performance testing results in both environments, we also investigate ways to transform results from virtual and physical environments and performance metrics based on deviance may reduced the discrepancy between performance metrics. Overall, we recommend that practitioners should not simply assume that performance testing results done on virtual environments will be the same in physical environments.

\end{abstract}

\begin{acknowledgments}

Thesis acknowledgments.

\end{acknowledgments}

\begin{publications}

The following publications are related to this thesis:

\begin{enumerate}

\item \textbf{...} 

%\item \textbf{Moiz}, 
\end{enumerate}

\end{publications}

\chapter{Introduction}
\label{introduction}
\input{introduction.tex}

\chapter{Literature Review}
\label{literature_review}
\input{literature_review.tex}

\chapter{Testing For The Ubiety Of Discrepancy}
\label{chapter3}
\input{chapter3.tex}

\chapter{Studying The Impact Of Modifying The Virtual Environment}
\label{chapter4}
\input{chapter4.tex}

\chapter{Summary, Contributions and Future Work}
\label{conclusion}
\input{conclusion.tex}

%\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{bibliography.bib}  
\bibliographystyle{alpha}
\end{document}