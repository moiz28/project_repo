%Cloud computing has eliminated the need for hardware decisions. It is now a significant part of the IT industry incorporating not only the leading names like \textit{Facebook, Amazon, Google} etc but also small-scaled business ventures that are migrating on to a cloud based environment. According to a survey \textit{'survey'}, up to 70\% of the companies are moving from conventional physical data servers? to cloud based servers. The primary concerns when migrating are, usually, costs, availability performance etc of the system. 

%Software Performance Engineering(SPE) incorporates all the software engineering activities carried out during the project's lifecycle required to meet the software's performance requirements.
%These performance requirements ensure that the user requests are served are in a timely manner and the software's response time does not degrade with the increase in workload i.e. users. In the midst of todays large-scale software systems, (e.g.Amazon and Google's Gmail), SPE plays a vital role as most of the failures are associated with performance than with feature bugs. A second's downtime of amazon.com would cost millions of dollars. One of the recent examples in this regard was the roll-out failure of healthcare.gov. Another example was the crash of Facebook which caused NASDAQ a high monetary misfortune. 

%To avoid performance failures, software performance engineers use performance testing. The tests are used to exercise the subject system which is under test. To exercise this system, it is subjected to different workloads. These tests are designed to uncover performance bottlenecks or a test objective like maximum operational capacity. 

%The goal of performance testing is to test how the system responds are realistic workloads. Therefore performance test are composted of test workload and configuration of SUT which represents a field like environment. 

%The goal of this study is to compare the performance of a SUT in heterogeneous environments. Due to lack of resources and constant requirement of evolution of the testing environment practitioners often rely on testing done in virtual environments. However, the road that leads to the reliability of performance activities done in virtual environments still remains undiscovered. 

Software Performance Engineering (SPE) engulfs all the performance assurance activities carried out during the complete development life cycle of the project. These activities make sure that the software meets the desired performance requirements~\cite{futureofspe}. SPE plays a vital role in the software industry. A performance failure in large scale system are not uncommon~\cite{tailatscale, foo2010mining}, leading to not only an eventual decline in the quality of the system but also monetary and temporal losses~\cite{costofdowntime}. For example, an interruption in Amazon Web Service(2011) lead to the disruption of Quora, Reddit, Foursquare and numerous other web sites~\cite{amazondown}. Amazon calculated that a second's downtime can cost up to \$1.6 billion(cite here/problem). Or the famous inefficient roll-out of healthcare.gov~\cite{healthcare} leading to serious reputational issues.

%In the software ecosystem, performance assurance activities play a vital role \cite{Shang:2015:ADP:2668930.2688052}. In essence, these activities ensure a consistent software functionality. The trend of dedicating a large chunk of costs, in some cases even exceeding the cost of development \cite{bertolino2007software} to such performance assurance activities is now not unusual. In fact, most of the problems in the field are due to performance related issues \cite{foo2010mining} . A failure here would not only include an eventual decline in the quality of the software but also monetary and temporal losses. That is why companies like \textit{Facebook, Amazon and Google} are committed to achieve excellence in this regard. \cite{jackson2010performance}

To mitigate the performance failures, performance analysts often test the subject system with performance tests in order to study and analyze the system~\cite{futureofspe}. Performance tests are subjected to highlight system's performance which are congruent to a field-like load~\cite{Shang:2015:ADP:2668930.2688052}. For example, to investigate the performance bottlenecks, the maximum throughput of the system~\cite{syer2014maintenance} or other the non-functional performance requirements.

%The objective behind performance regression testing is to identify if there exists a lapse in performance for the newer version of the software compared to the previous versions. The system is tested by applying a fixed load which is congruent to a field-like load \cite{Shang:2015:ADP:2668930.2688052} \cite{foo2010mining} \cite{5306331}. The performance analysts then look for deviations between metrics values compared to the earlier versions. Examples of factors causing performance lapse may be because of high CPU utilization or a memory leak. \cite{5306331}. As there are no benchmarks for measuring the software performance cross environments, and with little or no time dedicated to performance assurance activities practitioners often find it hard to test and analyze the results of regression testing.

The need for performance testing environments to advance and evolve is continually augmenting and so is the cost associated with it~\cite{stpmag, bertolino2007software}. Consequently, one of the key problems practitioners face is lack of resources available for performance assurance activities~\cite{5306331}. For example, a lack of systematically recorded set of baselines tests or hefty sets of data in the form of performance metrics. More importantly, performance testing is often pushed to the last stage of the software development lifecycle which forces the managers to dedicate a minimal time for performance testing which can even span out to days. That's why practitioners prefer testing the system in a virtual environment~\cite{whyvirtualisbetter, vmwarehighcost}. The choice of running the performance assurance activities in a virtual environment is also based on the complexity of the large scale software systems. This enforces a virtual set up of the environment which saves resources and is easier to set up according to the desired needs~\cite{VMWarePowerCLIBlog, seetharaman2006test}. 

Regrettably, the road that leads to a comparison between the performance of the system in a physical and virtual environment remains undiscovered. Whether virtual environments are applicable in performance assurance activities or if they can be relied to behave equivalent to the physical servers still remains questionable. There have been limited instances of diagnosis of performance overheads~\cite{menon2005diagnosing} in the domain of performance testing however no concrete conclusions have been drawn yet. 

The goal of our study is evaluating the discrepancy between the performance testing results generated from physical and virtual environments. 
%Additionally, we investigated if the performane tests done in virtual environments are repeatable or not. 
%Additionally, if they do not belong to the same population, then how impactful and effective are the prevalent discrepancies for a model-based regression testing approach.    
We conduct two case studies on two open-source systems, DS2~\cite{delldvd} and CloudStore~\cite{cloudstore} by running performance tests in both physical and virtual environment. We compare the performance test results that are generated from both the environments.

In particular, we first compare the performance metrics values and distributions. We then investigate the correlation of performance counters to the generated load. Finally, we build statistical models to help us reach conclusions. 
We observed that majority of the performance counters do not belong to the same family of distribution. We also observed that the metric that are highly correlated with the load are not the same across both our subject systems.
We concluded that due to the discrepancies present in the virtual environment, we can not rely on performance evaluated in the virtual environment as is. We then apply scaling techniques to the metrics generated in the virtual environment to build linear regression models. We concluded that the performance metrics in the virtual environment are not identical copies of the metrics in the physical environment. 


% to determine and compare the distributions. This is achieved by comparing plots and finding correlation between the metrics cross-environments. We use generalized linear regression models, as used in the performance assurance activities \cite{Shang:2015:ADP:2668930.2688052}, to determine the extent of the comparison between metrics. If the metrics are transferable between the environments, we should expect to see a low percentage error. We chose to build our regression models based on multiple performance metrics to see the effect of the clustered performance metrics. 

%We diffuse our findings by answering the following research questions:

%\begin{description}
%	\item[$\bullet$] RQ1: Do performance metrics from physical and virtual environments belong to the same population?
%	\item[$\bullet$] RQ2: Is the metric correlation with load same cross-environments?
%	\item[$\bullet$] RQ3: Do performance metrics from different environments impact performance modeling?
	
%\end{description}

The rest of the paper is organized as follows. In Section~\ref{sec:related_work}, we discuss background and related work. Section~\ref{~\ref{sec:case_study_setup} } discusses the approach and the statistical techniques we adopted for this study. We present our case study results in Section\ref{sec:results}. Followed by discussion in section~\ref{sec:discussion} and threats to validity in Section~\ref{sec:threats_to_validity}. Section~\ref{sec:conclusion} concludes this paper.
