%Cloud computing has eliminated the need for hardware decisions. It is now a significant part of the IT industry incorporating not only the leading names like \textit{Facebook, Amazon, Google} etc but also small-scaled business ventures that are migrating on to a cloud based environment. According to a survey \textit{'survey'}, up to 70\% of the companies are moving from conventional physical data servers? to cloud based servers. The primary concerns when migrating are, usually, costs, availability performance etc of the system. 

%Software Performance Engineering(SPE) incorporates all the software engineering activities carried out during the project's lifecycle required to meet the software's performance requirements.
%These performance requirements ensure that the user requests are served are in a timely manner and the software's response time does not degrade with the increase in workload i.e. users. In the midst of todays large-scale software systems, (e.g.Amazon and Google's Gmail), SPE plays a vital role as most of the failures are associated with performance than with feature bugs. A second's downtime of amazon.com would cost millions of dollars. One of the recent examples in this regard was the roll-out failure of healthcare.gov. Another example was the crash of Facebook which caused NASDAQ a high monetary misfortune. 

%To avoid performance failures, software performance engineers use performance testing. The tests are used to exercise the subject system which is under test. To exercise this system, it is subjected to different workloads. These tests are designed to uncover performance bottlenecks or a test objective like maximum operational capacity. 

%The goal of performance testing is to test how the system responds are realistic workloads. Therefore performance test are composted of test workload and configuration of SUT which represents a field like environment. 

%The goal of this study is to compare the performance of a SUT in heterogeneous environments. Due to lack of resources and constant requirement of evolution of the testing environment practitioners often rely on testing done in virtual environments. However, the road that leads to the reliability of performance activities done in virtual environments still remains undiscovered. 

Software performance engineering engulfs performance assurance activities during the development and operation of software systems. These activities ensure that the software meets the desired performance requirements~\cite{futureofspe}. Software performance engineering plays a vital role in the reliability of large software systems in the field. Failures in large software systems that are due to performance issues are rather common~\cite{tailatscale, foo2010mining}, leading to not only an eventual decline in the quality of the system but also monetary and temporal losses~\cite{costofdowntime}. For instance, an interruption in the Amazon Web Service lead to the disruption of Quora, Reddit, Foursquare and numerous other web sites~\cite{amazondown}. Amazon estimates that a one-second page-load slowdown can cost up to \$1.6 billion~\cite{amazononesec}. The impact from performance issue on system reliability would also lead to serious reputational issues.

%In the software ecosystem, performance assurance activities play a vital role \cite{Shang:2015:ADP:2668930.2688052}. In essence, these activities ensure a consistent software functionality. The trend of dedicating a large chunk of costs, in some cases even exceeding the cost of development \cite{bertolino2007software} to such performance assurance activities is now not unusual. In fact, most of the problems in the field are due to performance related issues \cite{foo2010mining} . A failure here would not only include an eventual decline in the quality of the software but also monetary and temporal losses. That is why companies like \textit{Facebook, Amazon and Google} are committed to achieve excellence in this regard. \cite{jackson2010performance}

In order to mitigate the performance issues to ensure software reliability, practitioners often conduct performance tests~\cite{futureofspe}. During performance tests, a workload (e.g., mimicking users' behaviour in the field) is applied on the software system~\cite{ranjanbook,Syer2016}. By analyzing performance metrics, such as CPU usage, that are generated from the test, practitioners can evaluate the performance of the software systems and identify potential performance issues (like memory leak~\cite{markicsm2013}) and bottlenecks~\cite{5635038}.

 %Performance tests are subjected to highlight system's performance which are congruent to a field-like load~\cite{Shang:2015:ADP:2668930.2688052, Syer2016}. For example, to investigate the performance bottlenecks, the maximum throughput of the system~\cite{syer2014maintenance} or other the non-functional performance requirements.

%The objective behind performance regression testing is to identify if there exists a lapse in performance for the newer version of the software compared to the previous versions. The system is tested by applying a fixed load which is congruent to a field-like load \cite{Shang:2015:ADP:2668930.2688052} \cite{foo2010mining} \cite{5306331}. The performance analysts then look for deviations between metrics values compared to the earlier versions. Examples of factors causing performance lapse may be because of high CPU utilization or a memory leak. \cite{5306331}. As there are no benchmarks for measuring the software performance cross environments, and with little or no time dedicated to performance assurance activities practitioners often find it hard to test and analyze the results of regression testing.

The need for performance testing environments to advance and evolve is continually augmenting and so is the cost associated with the environment~\cite{stpmag, bertolino2007software}. One of challenges practitioners face is the lack of available resources for performance testing. For instance, performance testing often need to run for a long period of time in order to build statistical confidence on the results~\cite{ranjanbook}. In addition, such testing environment needs to be carefully configured to avoid falsely shown performance issues due to the environmental setup. Making it worse, the diversified and ever-changing users' behaviour forces the testing environments to be frequently customized and updated~\cite{Syer2016}. To address such challenges, virtual environments are often leverage for performance testing~\cite{whyvirtualisbetter, vmwarehighcost}. The flexibility of virtual environments assist practitioners prepare, customize and update performance testing environments in a timely manner.


%More importantly, performance testing is often last stage of the software development lifecycle which forces the managers to dedicate a minimal time for performance testing which can even span out to days. That's why practitioners prefer testing the system in a virtual environment~\cite{whyvirtualisbetter, vmwarehighcost}. The choice of running the performance assurance activities in a virtual environment is also based on the complexity of the large scale software systems. This enforces a virtual set up of the environment which saves resources and is easier to set up according to the desired needs~\cite{VMWarePowerCLIBlog, seetharaman2006test}. 


However, research on virtual machine unveils the hidden overheads in virtual environments~\cite{menon2005diagnosing}, leading to possible discrepancy in performance testing results. Yet, virtual environments are highly leveraged in practice and in prior research~\cite{Nguyen:2012:ADP:2188286.2188344,xiong2013vperfguard} without any consideration of such overheads. To the best of our knowledge, the discrepancy between performance testing results in physical and virtual environment remains undiscovered. Evaluating and quantifying such discrepancy would help practitioners and researchers in the area of software performance and reliability better understand their performance testing results.

%Whether virtual environments are applicable in performance assurance activities or if they can be relied to behave equivalent to the physical servers still remains questionable. There have been limited instances of diagnosis of performance overheads~\cite{menon2005diagnosing} in the domain of performance testing however no concrete conclusions have been drawn yet. 

%The goal of our study is evaluating the discrepancy between the performance testing results from virtual and physical environments. 
%Additionally, we investigated if the performane tests done in virtual environments are repeatable or not. 
%Additionally, if they do not belong to the same population, then how impactful and effective are the prevalent discrepancies for a model-based regression testing approach.    
In this paper, we conduct a case study on two open-source systems, DS2~\cite{delldvd} and CloudStore~\cite{cloudstore} by running performance tests in both virtual and physical environments. We compare the performance test results that are generated from both the environments. In particular, we compare the performance testing results by 1) examining individual performance metric, 2) examining relationship among performance metrics and 3) building statistical models using performance metrics. 

We find that performance metrics typically do not follow the same distribution nor same trend in virtual and physical environments. We leverage a heatmap to visualize the changes in correlations among performance metrics and the system load metric. We observe large changes in correlations among I/O related metrics in two environments. Finally, we find that the statistical models using performance metrics cannot apply on the performance metrics collected from another environments (with high prediction error), even after normalizing the performance metrics. However, in some cases, normalizing performance metrics based on deviance may reduce prediction error when applying on the performance metrics collected from another environments. Practitioners who want to leverage both virtual and physical environments may consider using such normalization technique to reduce the discrepancy. Our findings highlight the need of awareness of the discrepancy between performance testing results in virtual and physical environments, and the need to research efforts on investigating how to improve the use of both virtual and physical environments to ensure system reliability.

%In particular, we first compare the performance metrics values and distributions. We then investigate the correlation of performance counters to the generated load. Finally, we build statistical models to help us reach conclusions. 
%We observed that majority of the performance counters do not belong to the same family of distribution. We also observed that the metric that are highly correlated with the load are not the same across both our subject systems.
%We concluded that due to the discrepancies present in the virtual environment, we can not rely on performance evaluated in the virtual environment as is. We then apply scaling techniques to the metrics generated in the virtual environment to build linear regression models. We concluded that the performance metrics in the virtual environment are not identical copies of the metrics in the physical environment. 


% to determine and compare the distributions. This is achieved by comparing plots and finding correlation between the metrics cross-environments. We use generalized linear regression models, as used in the performance assurance activities \cite{Shang:2015:ADP:2668930.2688052}, to determine the extent of the comparison between metrics. If the metrics are transferable between the environments, we should expect to see a low percentage error. We chose to build our regression models based on multiple performance metrics to see the effect of the clustered performance metrics. 

%We diffuse our findings by answering the following research questions:

%\begin{description}
%	\item[$\bullet$] RQ1: Do performance metrics from physical and virtual environments belong to the same population?
%	\item[$\bullet$] RQ2: Is the metric correlation with load same cross-environments?
%	\item[$\bullet$] RQ3: Do performance metrics from different environments impact performance modeling?
	
%\end{description}

The rest of the paper is organized as follows. Section~\ref{sec:related_work} presents the background and related work to this paper. Section~\ref{sec:case_study_setup} presents the case study step. Section~\ref{sec:results} the results of our cases study, followed by discussion of our results in Section~\ref{sec:discussion}. Section~\ref{sec:threats_to_validity} discusses the threats to validity of our findings. Finally, Section~\ref{sec:conclusion} concludes this paper.
