To the best of our knowledge, the extent of the related work comparing the performance assurance activities carried out in the physical and virtual environments is limited. In this section, the related work we discuss use statistical techniques to detect performance regression on virtual environments only. 

\subsection{VM overheads}

Kraft \textit{et al.} \cite{kraft2011io} discuss the issues relative to performance modeling of disk I/O. They examine the performance degradation of disk request response time by recommending a trace-driven approach. Kraft \textit{et al.} \cite{kraft2011io} emphasize on the latencies existing in virtual machines request for disc IO due to increment in time associated with request queues. 

Aravind \textit{et al.} \cite{menon2005diagnosing} audit the performance overheads in Xen virtual machines. They uncover the origins of overheads that might exist in the network I/O causing a peculiar system behavior. However, there study is limited to Xen virtual machine only while mainly focusing on network related performance overheads.

\subsection{Performance regression detection} 

Shang\textit{ et al.} \cite{Shang:2015:ADP:2668930.2688052} came up with a methodology to counter the approach of including only a limited number of performance metrics for the performance regression models. They recommend to use a an automatic clustering technique in order to select a subset of performance metrics out of the entire set of metrics. Ensued by building peformance models for each cluster. These models engulf the relationships of performance counters within each cluster. Shang\textit{ et al.} \cite{Shang:2015:ADP:2668930.2688052} also demonstrated that their approach is applicable to a system with injected performance regression. We use the same technique in our study to inject performance regression in the target system nonetheless the limitation of their study to perform their experiments in a virtual environment persists.

Belonging to the same background in the domain of performance engineering is pair-wise analysis. Nguyen \textit{et al.} \cite{Nguyen:2012:ADP:2188286.2188344} introduce the concept of using control charts in order to detect performance regression, providing a solution for keeping numerous counters, only to make the tasks like book keeping and data analysis tedious for the practitioners. Control charts use a predefined threshold to detect performance anomalies. However control charts assume that the output follow a uni-modal distribution which an inappropriate assumption for the performance load. Nguyen \textit{ et al.} propose an approach to scale the metrics accordingly. However, the experiments are only carried out on the virtual machines in contrast to our approach.

Model-based approached use the target counters(e.g. DISC I/O and CPU Utilization) to build models and these models are then used to detect performance regression int he system. What make the model-based approach celebrated is the inclusion and comparison of numerous performance counters at the same time. 
Xiong \textit{et al.} \cite{xiong2013vperfguard} proposed a model-driven approach to sofdetect software performance regression. The devised framework called \textit{vPerfGuard} helps in detecting performance anomalies in a cloud-environment. Xiong \textit{et al.} \cite{xiong2013vperfguard} only used virtual environments to test their framework, therefor our study is crucial to such state of the art research as it lays down the platform for putting confidence in such performance assurance practices.
Jiang \textit{et al. }\cite{jiang2011system} used an improved least square regression models to detect system faults.
Cohen \textit{et al. }\cite{cohen2004correlating} adapted an approach that includes fabricating probabilistic model, e.g. Tree-Augmented Bayesian Networks, to examine the causes that target the changes in the system's response time. Cohen \textit{et al. }\cite{Cohen:2005:CIC:1095810.1095821} also proposed that system faults can be detected by building statistical models based on performance metrics. The works of Cohen \textit{et al: }'s \cite{cohen2004correlating} \cite{Cohen:2005:CIC:1095810.1095821} were improved by Bodik \textit{et al.} \cite{bodik2008hilighter} by using logistic regression models.



%Model-based investigation fabricates a set number of models for an arrangement of target execution counters (e.g., CPU and memory) and influences the models to identify execution relapses. The model-based methodology offers us some assistance with dealing with the expansive number of execution counters and aids in comparing the connections between the different counters. 
%Latest research by Xiong et al: proposes a model driven system to help with performance determination in a cloud environment. Their system manufactures models between workload counters and an chosen performance counter, such as CPU. The models can be utilized to recognize workload changes, and additionally helps with distinguishing execution bottlenecks.
%Cohen et al propose a methodology that fabricates probabilistic models, for example, Tree-Augmented Bayesian Networks, to associate framework level counters and frameworks' response time. The methodology is utilized to comprehend the reason to changes on systems' execution time. Cohen et al: propose 
%that execution counters can be utilized to assemble measurable models for framework issues. Bodik et al: use logistic regression models to enhance Cohen et al's: work
%Jiang et al: propose a methodology that ascertains the relationship between execution counters by enhancing the. Conventional Least Squares relapse models and utilizing the model to identify deficiencies in a framework. 
%Current model-based methodologies still have their limitations. Execution experts frequently select the objective performance counters in light of their experience and hunch. 
%They frequently concentrate on a little arrangement of surely understood counters (e.g., CPU and memory). Such specially appointed determination of target counters 
%may prompt the inability to watch execution relapses 
%

Previous literature has its limitations as most of the performance regression testing and regression modelling  is performed in virtual environments. Through our work we validate the usage of virtual environments in the field of performance engineering. 



