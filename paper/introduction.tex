%Cloud computing has eliminated the need for hardware decisions. It is now a significant part of the IT industry incorporating not only the leading names like \textit{Facebook, Amazon, Google} etc but also small-scaled business ventures that are migrating on to a cloud based environment. According to a survey \textit{'survey'}, up to 70\% of the companies are moving from conventional physical data servers? to cloud based servers. The primary concerns when migrating are, usually, costs, availability performance etc of the system. 

In the software ecosystem, performance assurance activities play a vital role \cite{Shang:2015:ADP:2668930.2688052}. In essence, these activities ensure a consistent software functionality. The trend of dedicating a large chunk of costs, in some cases even exceeding the cost of development \cite{bertolino2007software} to such performance assurance activities is now not unusual. In fact, most of the problems in the field are due to performance related issues \cite{foo2010mining} . A failure here would not only include an eventual decline in the quality of the software but also monetary and temporal losses. That is why companies like \textit{Facebook, Amazon and Google} are committed to achieve excellence in this regard. \cite{jackson2010performance}

The objective behind performance regression testing is to identify if there exists a lapse in performance for the newer version of the software compared to the previous versions. The system is tested by applying a fixed load which is congruent to a field-like load \cite{Shang:2015:ADP:2668930.2688052} \cite{foo2010mining} \cite{5306331}. The performance analysts then look for deviations between metrics values compared to the earlier versions. Examples of factors causing performance lapse may be because of high CPU utilization or a memory leak. \cite{5306331}. As there are no benchmarks for measuring the software performance cross environments, and with little or no time dedicated to performance assurance activities practitioners often find it hard to test and analyze the results of regression testing.

The need for performance testing environments to advance and evolve is continually augmenting and so is the cost associated with it \cite{stpmag} \cite{bertolino2007software}. Consequently, one of the key problems practitioners face is lack of resources available for performance assurance activities in the dedicated or physical environment. That's why testing the system in a virtual environment comes to the foreground \cite{vmwarehighcost}. Before the system is deployed in the dedicated environment, a performance engineer tests the newer version of the software to look for performance loopholes in the virtual environment. The choice of running the performance assurance activities in a virtual machine is also based on the fact that majority of the client-server based systems are complex \cite{BumHyunLim}. This enforces a virtual set up of the environment which saves resources and is easier to refurbish according to the desired needs \cite{VMWarePowerCLIBlog}\cite{seetharaman2006test}  

Regrettably, the road that leads to a comparison between the performance of the system in a dedicated and virtual environment remains undiscovered. Whether virtual machines are applicable in performance assurance activities or if they can be relied to behave equivalent to the physical servers still remains questionable. There have been limited instances of diagnosis of performance overheads \cite{menon2005diagnosing} in the domain of performance engineering however no concrete conclusions have been drawn yet. 

The goals of our study are to examine whether the performance metrics generated from our physical and virtual environments are homogeneous and correlated. 
%Additionally, we investigated if the performane tests done in virtual environments are repeatable or not. 
Additionally, if they do not belong to the same population, then how impactful and effective are the prevalent discrepancies for a model-based regression testing approach.    

To study this we set up two subject systems on an identical physical and a virtual environment. We use the performance metrics generated from both the environments to determine and compare the distributions. This is achieved by comparing plots and finding correlation between the metrics cross-environments. We use generalized linear regression models, as used in the performance assurance activities \cite{Shang:2015:ADP:2668930.2688052}, to determine the extent of the comparison between metrics. If the metrics are transferable between the environments, we should expect to see a low percentage error. We chose to build our regression models based on multiple performance metrics to see the effect of the clustered performance metrics. 

We diffuse our findings by answering the following research questions:

\begin{description}
	\item[$\bullet$] RQ1: Do performance metrics from physical and virtual environments belong to the same population?
	\item[$\bullet$] RQ2: Is the metric correlation with load same cross-environments?
	\item[$\bullet$] RQ3: Do performance metrics from different environments impact performance modeling?

\end{description}

This paper is organized as follows. In section 2 we discuss background and related work. Section 3 demonstrates an example serving as a motivation for this paper. Section 4 discusses the approach and the statistical techniques we adopted for this study. We present our case study results in section 5. Followed by discussion in section 6 and threats to validity in section 7. Section 8 concludes this paper.
